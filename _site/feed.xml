<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-17T00:28:31+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Piotr Trochim’s blog</title><subtitle>Thoughts and results I wish to persist.
</subtitle><entry><title type="html">Bayesian Inference</title><link href="http://localhost:4000/math/2020/05/16/bayesian-inference.html" rel="alternate" type="text/html" title="Bayesian Inference" /><published>2020-05-16T00:00:00+01:00</published><updated>2020-05-16T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/05/16/bayesian-inference</id><content type="html" xml:base="http://localhost:4000/math/2020/05/16/bayesian-inference.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.&lt;/p&gt;

&lt;p&gt;In this post, I will be covering a method that builds on top of &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Maximum Likelihood Estimate&lt;/a&gt; method I covered in &lt;a href=&quot;https://paksas.github.io/math/2020/05/15/mle.html&quot;&gt;the previous post&lt;/a&gt;, enhancing it.&lt;/p&gt;

&lt;h1 id=&quot;back-alley-clown&quot;&gt;Back alley clown&lt;/h1&gt;

&lt;p&gt;Let me recall the example from the previous post.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.&lt;/p&gt;

  &lt;p&gt;You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Back then though, all we cared about was the result of those 10 throws. But that’s not how humans work. We take circumstance into account.
I would be far more likely to respond to the challenge if I was attending a fair than if I was making a shortcut through a dark, dirty alley.&lt;/p&gt;

&lt;p&gt;I’m of course describing the process of &lt;em&gt;building confidence&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A fair clown is something rather desired, whereas one encounteres in a back alley would have to first gain our trust. His coin tosses would have to be pretty &lt;em&gt;convincing&lt;/em&gt;, showing no bias towards one side or another.&lt;/p&gt;

&lt;h1 id=&quot;two-distributions&quot;&gt;Two distributions&lt;/h1&gt;

&lt;p&gt;Mathematically speaking, we are still looking for the parameters of a distribution that models the Random Variable that is the clown’s coin.&lt;/p&gt;

&lt;p&gt;However now in addition to solid evidence, we also have our &lt;em&gt;prior&lt;/em&gt; beliefs. Those two combined allow us not only to make a guess about the parameter value, but also reflect our level of &lt;em&gt;confidence&lt;/em&gt; in that guess.&lt;/p&gt;

&lt;p&gt;If you recall, MLE allowed to direcly calculate the values of distribution parameters. It meant that it was caclulating a distribution with &lt;em&gt;complete confidence&lt;/em&gt; in its result. It makes sense, since the method relied exclusively on the data, which is a solid evidence. So unless there was other data available, the method was 100% confident about its estimate.&lt;/p&gt;

&lt;p&gt;Right now we want to express our &lt;em&gt;doubt&lt;/em&gt;, and &lt;em&gt;doubts&lt;/em&gt; are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to &lt;em&gt;sample&lt;/em&gt; the values of our &lt;em&gt;target distribution&lt;/em&gt; parameters.&lt;/p&gt;

&lt;p&gt;It took me a very long time to come to that realization during my studies, so I want to repeat that &lt;em&gt;important&lt;/em&gt; paragraph again:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Right now we want to express our &lt;em&gt;doubt&lt;/em&gt;, and &lt;em&gt;doubts&lt;/em&gt; are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to &lt;em&gt;sample&lt;/em&gt; the values of our &lt;em&gt;target distribution&lt;/em&gt; parameters.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;building-a-model-of-uncertainty&quot;&gt;Building a model of uncertainty&lt;/h1&gt;

&lt;p&gt;We have two quantities now:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the observed coin toss results&lt;/li&gt;
  &lt;li&gt;the level of confidence about the parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can express confidence about something with a distribution. Let’s denote that distribution &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;, without defining it really.
Ditributions are properties of Random Variables - and this one is no different. Its Random Variable samples from a set of all possible distribution parameters &lt;script type=&quot;math/tex&quot;&gt;\theta \in \omega&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Think about it - we now have a random variable that has some expected value and variance (uncertainty about that value) that represents the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; of our &lt;em&gt;target distribution&lt;/em&gt;. This is exactly what we wanted.&lt;/p&gt;

&lt;p&gt;But what about the data - the toss results?
Well, since we have access to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; values, we could check which ones can even generate the tosses we observed, and how likely would those be.&lt;/p&gt;

&lt;p&gt;Then, we could weight those predictions by the probability of actually drawing the specific parameter values &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Allow me to be loose with the notation and just start putting things together:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta, data) = f(data, \theta) * \xi(\theta)&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is a specific sample, and so is the &lt;script type=&quot;math/tex&quot;&gt;data&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt; would return the probability of drawing &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; from the &lt;em&gt;parameter distribution&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f(data, \theta)&lt;/script&gt; would return the probability of drawing &lt;script type=&quot;math/tex&quot;&gt;data&lt;/script&gt; for the given value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;and by that property, &lt;script type=&quot;math/tex&quot;&gt;p(\theta, data)&lt;/script&gt; would be the revised &lt;em&gt;“probability”&lt;/em&gt; of drawing parameter value &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; after having observed &lt;script type=&quot;math/tex&quot;&gt;data&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 3 problems with the equation above:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it operates on specific values, so it could be good maybe for discrete distributions, but would break as soon as we wanted to use a continuous one.&lt;/li&gt;
  &lt;li&gt;it operates on specific values (deuce), so it doesn’t really give us the distribution we are looking for&lt;/li&gt;
  &lt;li&gt;if &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt; was a discrete distribution, and we would actually iterate all values of &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt; and summed the returned probabilites, they wouldn’t add up to 1 - so it doesn’t actually return a probability distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayes-theorem&quot;&gt;Bayes Theorem&lt;/h1&gt;

&lt;p&gt;At this point I would like to introduce the Bayes Theorem. Actually all of you probably know it, and those who don’t can have a quick look at one of the numerous sources that describe it in detail, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;this Wikipedia page&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(A|B)=\frac{p(B|A) * p(A)}{p(B)}&lt;/script&gt;

&lt;p&gt;or, more precisely:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(A_j|B)=\frac{p(B|A_j) * p(A_j)}{\sum_{i} p(B|A_i) * p(A_i)}&lt;/script&gt;

&lt;p&gt;In plain English, our belief about particular parameter values A_j, after we have observed coin toss results B, is the same as the probability of seeing those results being drawn from a distribution parameterized using A_j, and then scaling that value by the likelihood of actually coming across the parameters A_j, as well as that of running into thoe toss results - no matter what parameterization is used.&lt;/p&gt;

&lt;h1 id=&quot;bayes-theorem-to-fix-our-model&quot;&gt;Bayes Theorem to fix our model&lt;/h1&gt;

&lt;p&gt;Our model doesn’t look much like the Bayes Theorem:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Bayes Theorem&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;p(\theta, data) = f(data, \theta) * \xi(\theta)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;p(A|B)=\frac{p(B|A) * p(A)}{p(B)}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Even if we replace A and B with the same letters, they are still different:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Bayes Theorem&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;p(\theta, data) = f(data, \theta) * \xi(\theta)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;p(\Theta|Data)=\frac{p(Data|\Theta) * p(\Theta)}{p(Data)}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;we don’t have the denominator &lt;script type=&quot;math/tex&quot;&gt;p(Data)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Bayes Theorem uses Random Variables (upper case labels), where as our model operates on samples (lower case labels)&lt;/li&gt;
  &lt;li&gt;the functions look kind of the same, but the ones in our model are missing the conditional vertical bars&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conjugate-distribution&quot;&gt;Conjugate distribution&lt;/h1&gt;

&lt;p&gt;Explain that in order to be able to use posterior as the next iteration prior, MLE and prior need to be conjugate distributions
Quote Pearse stats book on the conjugate prior
Beta is a conjugate of the binomial distr.&lt;/p&gt;

&lt;h1 id=&quot;posterior&quot;&gt;Posterior&lt;/h1&gt;

&lt;h1 id=&quot;algebra-to-derive-the-posterior-update-rule&quot;&gt;Algebra to derive the posterior update rule&lt;/h1&gt;

&lt;h1 id=&quot;example&quot;&gt;Example&lt;/h1&gt;

&lt;p&gt;Interactive plot showing a few iterations in which we update the initial belief&lt;/p&gt;

&lt;p&gt;here’s my plot&lt;/p&gt;

&lt;html lang=&quot;en&quot;&gt;
  &lt;head&gt;
      &lt;meta charset=&quot;utf-8&quot; /&gt;
      &lt;title&gt;my plot&lt;/title&gt;  
        &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js&quot;&gt;&lt;/script&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            Bokeh.set_log_level(&quot;info&quot;);
        &lt;/script&gt;
        
      
      
    
  &lt;/head&gt;
  
  
  &lt;body&gt;
    
      
        
          
          
            
              &lt;div class=&quot;bk-root&quot; id=&quot;ca2a4bcc-13e4-4a2e-b5de-4cfc587902a5&quot; data-root-id=&quot;1001&quot;&gt;&lt;/div&gt;
            
          
        
      
      
        &lt;script type=&quot;application/json&quot; id=&quot;1146&quot;&gt;
          {&quot;85c49e6c-531d-4827-98bd-4723a9986045&quot;:{&quot;roots&quot;:{&quot;references&quot;:[{&quot;attributes&quot;:{&quot;bottom_units&quot;:&quot;screen&quot;,&quot;fill_alpha&quot;:{&quot;value&quot;:0.5},&quot;fill_color&quot;:{&quot;value&quot;:&quot;lightgrey&quot;},&quot;left_units&quot;:&quot;screen&quot;,&quot;level&quot;:&quot;overlay&quot;,&quot;line_alpha&quot;:{&quot;value&quot;:1.0},&quot;line_color&quot;:{&quot;value&quot;:&quot;black&quot;},&quot;line_dash&quot;:[4,4],&quot;line_width&quot;:{&quot;value&quot;:2},&quot;render_mode&quot;:&quot;css&quot;,&quot;right_units&quot;:&quot;screen&quot;,&quot;top_units&quot;:&quot;screen&quot;},&quot;id&quot;:&quot;1044&quot;,&quot;type&quot;:&quot;BoxAnnotation&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1025&quot;,&quot;type&quot;:&quot;HelpTool&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1045&quot;,&quot;type&quot;:&quot;Selection&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1024&quot;,&quot;type&quot;:&quot;ResetTool&quot;},{&quot;attributes&quot;:{&quot;fill_alpha&quot;:{&quot;value&quot;:0.1},&quot;fill_color&quot;:{&quot;value&quot;:&quot;#1f77b4&quot;},&quot;line_alpha&quot;:{&quot;value&quot;:0.1},&quot;line_color&quot;:{&quot;value&quot;:&quot;#1f77b4&quot;},&quot;x&quot;:{&quot;field&quot;:&quot;x&quot;},&quot;y&quot;:{&quot;field&quot;:&quot;y&quot;}},&quot;id&quot;:&quot;1035&quot;,&quot;type&quot;:&quot;Circle&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1016&quot;,&quot;type&quot;:&quot;BasicTicker&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1008&quot;,&quot;type&quot;:&quot;LinearScale&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1046&quot;,&quot;type&quot;:&quot;UnionRenderers&quot;},{&quot;attributes&quot;:{&quot;source&quot;:{&quot;id&quot;:&quot;1033&quot;,&quot;type&quot;:&quot;ColumnDataSource&quot;}},&quot;id&quot;:&quot;1037&quot;,&quot;type&quot;:&quot;CDSView&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1023&quot;,&quot;type&quot;:&quot;SaveTool&quot;},{&quot;attributes&quot;:{&quot;callback&quot;:null,&quot;data&quot;:{&quot;x&quot;:[1,2],&quot;y&quot;:[3,4]},&quot;selected&quot;:{&quot;id&quot;:&quot;1045&quot;,&quot;type&quot;:&quot;Selection&quot;},&quot;selection_policy&quot;:{&quot;id&quot;:&quot;1046&quot;,&quot;type&quot;:&quot;UnionRenderers&quot;}},&quot;id&quot;:&quot;1033&quot;,&quot;type&quot;:&quot;ColumnDataSource&quot;},{&quot;attributes&quot;:{&quot;dimension&quot;:1,&quot;ticker&quot;:{&quot;id&quot;:&quot;1016&quot;,&quot;type&quot;:&quot;BasicTicker&quot;}},&quot;id&quot;:&quot;1019&quot;,&quot;type&quot;:&quot;Grid&quot;},{&quot;attributes&quot;:{&quot;fill_color&quot;:{&quot;value&quot;:&quot;#1f77b4&quot;},&quot;line_color&quot;:{&quot;value&quot;:&quot;#1f77b4&quot;},&quot;x&quot;:{&quot;field&quot;:&quot;x&quot;},&quot;y&quot;:{&quot;field&quot;:&quot;y&quot;}},&quot;id&quot;:&quot;1034&quot;,&quot;type&quot;:&quot;Circle&quot;},{&quot;attributes&quot;:{&quot;active_drag&quot;:&quot;auto&quot;,&quot;active_inspect&quot;:&quot;auto&quot;,&quot;active_multi&quot;:null,&quot;active_scroll&quot;:&quot;auto&quot;,&quot;active_tap&quot;:&quot;auto&quot;,&quot;tools&quot;:[{&quot;id&quot;:&quot;1020&quot;,&quot;type&quot;:&quot;PanTool&quot;},{&quot;id&quot;:&quot;1021&quot;,&quot;type&quot;:&quot;WheelZoomTool&quot;},{&quot;id&quot;:&quot;1022&quot;,&quot;type&quot;:&quot;BoxZoomTool&quot;},{&quot;id&quot;:&quot;1023&quot;,&quot;type&quot;:&quot;SaveTool&quot;},{&quot;id&quot;:&quot;1024&quot;,&quot;type&quot;:&quot;ResetTool&quot;},{&quot;id&quot;:&quot;1025&quot;,&quot;type&quot;:&quot;HelpTool&quot;}]},&quot;id&quot;:&quot;1026&quot;,&quot;type&quot;:&quot;Toolbar&quot;},{&quot;attributes&quot;:{&quot;formatter&quot;:{&quot;id&quot;:&quot;1040&quot;,&quot;type&quot;:&quot;BasicTickFormatter&quot;},&quot;ticker&quot;:{&quot;id&quot;:&quot;1011&quot;,&quot;type&quot;:&quot;BasicTicker&quot;}},&quot;id&quot;:&quot;1010&quot;,&quot;type&quot;:&quot;LinearAxis&quot;},{&quot;attributes&quot;:{&quot;data_source&quot;:{&quot;id&quot;:&quot;1033&quot;,&quot;type&quot;:&quot;ColumnDataSource&quot;},&quot;glyph&quot;:{&quot;id&quot;:&quot;1034&quot;,&quot;type&quot;:&quot;Circle&quot;},&quot;hover_glyph&quot;:null,&quot;muted_glyph&quot;:null,&quot;nonselection_glyph&quot;:{&quot;id&quot;:&quot;1035&quot;,&quot;type&quot;:&quot;Circle&quot;},&quot;selection_glyph&quot;:null,&quot;view&quot;:{&quot;id&quot;:&quot;1037&quot;,&quot;type&quot;:&quot;CDSView&quot;}},&quot;id&quot;:&quot;1036&quot;,&quot;type&quot;:&quot;GlyphRenderer&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1011&quot;,&quot;type&quot;:&quot;BasicTicker&quot;},{&quot;attributes&quot;:{&quot;callback&quot;:null},&quot;id&quot;:&quot;1004&quot;,&quot;type&quot;:&quot;DataRange1d&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1020&quot;,&quot;type&quot;:&quot;PanTool&quot;},{&quot;attributes&quot;:{&quot;text&quot;:&quot;&quot;},&quot;id&quot;:&quot;1039&quot;,&quot;type&quot;:&quot;Title&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1006&quot;,&quot;type&quot;:&quot;LinearScale&quot;},{&quot;attributes&quot;:{&quot;ticker&quot;:{&quot;id&quot;:&quot;1011&quot;,&quot;type&quot;:&quot;BasicTicker&quot;}},&quot;id&quot;:&quot;1014&quot;,&quot;type&quot;:&quot;Grid&quot;},{&quot;attributes&quot;:{&quot;callback&quot;:null},&quot;id&quot;:&quot;1002&quot;,&quot;type&quot;:&quot;DataRange1d&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1021&quot;,&quot;type&quot;:&quot;WheelZoomTool&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1040&quot;,&quot;type&quot;:&quot;BasicTickFormatter&quot;},{&quot;attributes&quot;:{&quot;formatter&quot;:{&quot;id&quot;:&quot;1042&quot;,&quot;type&quot;:&quot;BasicTickFormatter&quot;},&quot;ticker&quot;:{&quot;id&quot;:&quot;1016&quot;,&quot;type&quot;:&quot;BasicTicker&quot;}},&quot;id&quot;:&quot;1015&quot;,&quot;type&quot;:&quot;LinearAxis&quot;},{&quot;attributes&quot;:{},&quot;id&quot;:&quot;1042&quot;,&quot;type&quot;:&quot;BasicTickFormatter&quot;},{&quot;attributes&quot;:{&quot;overlay&quot;:{&quot;id&quot;:&quot;1044&quot;,&quot;type&quot;:&quot;BoxAnnotation&quot;}},&quot;id&quot;:&quot;1022&quot;,&quot;type&quot;:&quot;BoxZoomTool&quot;},{&quot;attributes&quot;:{&quot;below&quot;:[{&quot;id&quot;:&quot;1010&quot;,&quot;type&quot;:&quot;LinearAxis&quot;}],&quot;center&quot;:[{&quot;id&quot;:&quot;1014&quot;,&quot;type&quot;:&quot;Grid&quot;},{&quot;id&quot;:&quot;1019&quot;,&quot;type&quot;:&quot;Grid&quot;}],&quot;left&quot;:[{&quot;id&quot;:&quot;1015&quot;,&quot;type&quot;:&quot;LinearAxis&quot;}],&quot;renderers&quot;:[{&quot;id&quot;:&quot;1036&quot;,&quot;type&quot;:&quot;GlyphRenderer&quot;}],&quot;title&quot;:{&quot;id&quot;:&quot;1039&quot;,&quot;type&quot;:&quot;Title&quot;},&quot;toolbar&quot;:{&quot;id&quot;:&quot;1026&quot;,&quot;type&quot;:&quot;Toolbar&quot;},&quot;x_range&quot;:{&quot;id&quot;:&quot;1002&quot;,&quot;type&quot;:&quot;DataRange1d&quot;},&quot;x_scale&quot;:{&quot;id&quot;:&quot;1006&quot;,&quot;type&quot;:&quot;LinearScale&quot;},&quot;y_range&quot;:{&quot;id&quot;:&quot;1004&quot;,&quot;type&quot;:&quot;DataRange1d&quot;},&quot;y_scale&quot;:{&quot;id&quot;:&quot;1008&quot;,&quot;type&quot;:&quot;LinearScale&quot;}},&quot;id&quot;:&quot;1001&quot;,&quot;subtype&quot;:&quot;Figure&quot;,&quot;type&quot;:&quot;Plot&quot;}],&quot;root_ids&quot;:[&quot;1001&quot;]},&quot;title&quot;:&quot;Bokeh Application&quot;,&quot;version&quot;:&quot;1.4.0&quot;}}
        &lt;/script&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('1146').textContent;
                  var render_items = [{&quot;docid&quot;:&quot;85c49e6c-531d-4827-98bd-4723a9986045&quot;,&quot;roots&quot;:{&quot;1001&quot;:&quot;ca2a4bcc-13e4-4a2e-b5de-4cfc587902a5&quot;}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts &gt; 100) {
                          clearInterval(timer);
                          console.log(&quot;Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing&quot;);
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != &quot;loading&quot;) fn();
            else document.addEventListener(&quot;DOMContentLoaded&quot;, fn);
          })();
        &lt;/script&gt;
    
  &lt;/body&gt;
  
&lt;/html&gt;

&lt;p&gt;after the plot&lt;/p&gt;</content><author><name>ptrochim</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Maximum Likelihood Estimate</title><link href="http://localhost:4000/math/2020/05/15/mle.html" rel="alternate" type="text/html" title="Maximum Likelihood Estimate" /><published>2020-05-15T00:00:00+01:00</published><updated>2020-05-15T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/05/15/mle</id><content type="html" xml:base="http://localhost:4000/math/2020/05/15/mle.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.&lt;/p&gt;

&lt;p&gt;In this post, I will be covering a method that relies exclusively on the data and doesn’t require the user to use any auxiliary distributions - Maximum Likelihood Estimate.&lt;/p&gt;

&lt;p&gt;I will assume that you are familiar with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The concept of a random variable&lt;/li&gt;
  &lt;li&gt;Binomial distribution&lt;/li&gt;
  &lt;li&gt;Expected value of a random variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;is-this-a-fair-coin&quot;&gt;Is this a fair coin?&lt;/h1&gt;

&lt;p&gt;Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.&lt;/p&gt;

&lt;p&gt;You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.&lt;/p&gt;

&lt;h1 id=&quot;what-you-see-is-all-there-is-wysiati&quot;&gt;What You See Is All There Is (WYSIATI)&lt;/h1&gt;

&lt;p&gt;This famous mnemonic coined by Daniel Kahneman in his book &lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&quot;&gt;“Thinking Fast and Slow”&lt;/a&gt; gives a hint how to proceed.&lt;/p&gt;

&lt;p&gt;What would be your guess if you have observed 10 Heads, and you had no other information to go on?&lt;/p&gt;

&lt;p&gt;If you did’t see the greedy smirk on the clown’s face, nor the long face of the person walking away from the table. You only saw the clown toss 10 Heads in a row.
What would be the result of the next one?&lt;/p&gt;

&lt;p&gt;Clearly, it’d be Heads. And if he threw 10 Tails, you would be stupid not to have bet on Tails… would you?&lt;/p&gt;

&lt;p&gt;As these thoughts run through your head, you are trying to guess a certain property - what to &lt;em&gt;expect&lt;/em&gt; from the coin as you keep tossing it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;10 Heads - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 1&lt;/script&gt;, so bet Heads&lt;/li&gt;
  &lt;li&gt;10 Tails - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 0&lt;/script&gt;, so bet Tails&lt;/li&gt;
  &lt;li&gt;5 Heads and 5 Tails - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 0.5&lt;/script&gt;, so… make a guess :/&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We need to guess the &lt;em&gt;expected value&lt;/em&gt; of the Random Variable, otherwise known as the mean of its distribution.&lt;/p&gt;

&lt;h1 id=&quot;relating-data-to-a-random-variable&quot;&gt;Relating data to a Random Variable&lt;/h1&gt;

&lt;p&gt;Our coin tossing experiment is best represented by a Random Variable with a Binomial Distribution.&lt;/p&gt;

&lt;p&gt;A discrete Random Variable is a sampler that pulls items out of a set. How frequently will certain values show up is dictated by its probability (mass) function. And it’s expected value (or mean) indicates which particular item we can expect to see pulled out the most.&lt;/p&gt;

&lt;p&gt;But how do we reverse it? How do we guess the mean and parameterize the probability (mass) function given the data that has been pulled out?&lt;/p&gt;

&lt;h1 id=&quot;binomial-distribution-for-a-coin-tossing-problem&quot;&gt;Binomial distribution for a coin tossing problem&lt;/h1&gt;

&lt;p&gt;The expected value of R.V. (a.k.a the mean of its distribution) is a maxima of that R.V’s probability (mass) function.
The best way to find the maxima of a function is to solve its derivative for its roots !&lt;/p&gt;

&lt;p&gt;So let’s remind ourselves of Binomial distribution p.m.f:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\theta, n, k) = \binom{n}{k} \theta^{k} (1 - \theta)^{n-k}&lt;/script&gt;

&lt;p&gt;Where, in our case:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; - total number of coin tosses&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; - number of &lt;em&gt;positive&lt;/em&gt; tosses (in our case let’s assume those are represented by Heads)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; - a latent parameter that can be thought of as the fairness of our coin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Binomial distribution is a discrete distribution. The word discrete suggests something that’s not continuous, something we wouldn’t be able to differentiate, right?&lt;/p&gt;

&lt;h1 id=&quot;misleading-vocabulary&quot;&gt;Misleading vocabulary&lt;/h1&gt;

&lt;p&gt;When I think about a distribution, I think about its probability function.
If its support is a discrete domain, then it’s a discrete distribution. If it’s continuous - like a real numbers line for example - then the distribution is continuous.&lt;/p&gt;

&lt;p&gt;When I look at the probability mass function (they make you use the term mass to highlight the discretness of the distribution), I see three parameters:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Values&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;n \in {0, 1, ...}&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Discrete&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;k \in {0, 1, ..., n}&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Discrete&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;theta \in R&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Continuous&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We clearly deal with two discrete parameters, and a continuous one. So which is it then?&lt;/p&gt;

&lt;p&gt;It depends on the use case actually. It also depends on the distribution (some, line the Normal distribution, have only continuous params).
But the bottom line is that you can change the support of the function. In our case:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if you were interested in finding a probability of a series of coin tosses given a specific coin, identified by a specific &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, you would fix its value, and the support would become fully discrete.&lt;/li&gt;
  &lt;li&gt;if you were told what the results of the tosses were, and asked to guess the property of the coin, its &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, you would fix &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, and the continuous &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; would become its support.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Confusing, isn’t it. One function, so many different options and use cases.&lt;/p&gt;

&lt;p&gt;In our case however, these two parameters are fixed - we are given 10 observations (&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;), and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; of them are Heads. What we need to find is such value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; that will maximize our observations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;argmax f(\theta, n, k)&lt;/script&gt;

&lt;h1 id=&quot;solving-the-derivative&quot;&gt;Solving the derivative&lt;/h1&gt;

&lt;p&gt;In order to find the maxima, we’ll use the differential calculus - a battle hardened method for optimizing a function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(\theta, n, k)}{d\theta} = 0&lt;/script&gt;

&lt;p&gt;After &lt;a href=&quot;#algebraic-derivation-of-mle-for-bernoulli-distribution&quot;&gt;a little bit of algebra&lt;/a&gt;, we get &lt;script type=&quot;math/tex&quot;&gt;\theta = \frac{k}{n}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The result suggests that the parameter of Binomial Distribution’s probability mass function that maximizes the likelihood of observing clown’s coin toss results is… the ratio of obtained Heads &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; to the number of performed throws &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/p&gt;

&lt;h1 id=&quot;deeper-insight---the-average&quot;&gt;Deeper insight - the average&lt;/h1&gt;

&lt;p&gt;If we assumed that Tails are represented by 0, and Heads by a 1, and an &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; would represent &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; coin toss result, then &lt;script type=&quot;math/tex&quot;&gt;n = \sum_{i=0}^{n} x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Our result then becomes the &lt;em&gt;average&lt;/em&gt; value of the observed data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \frac{n}{k} = \frac{1}{k} \sum_{i=0}^{n} x_i&lt;/script&gt;

&lt;h1 id=&quot;maximum-likelihood-estimate&quot;&gt;Maximum Likelihood Estimate&lt;/h1&gt;

&lt;p&gt;The method the relies on finding the maxima of a probability mass/density function is called Maximum Likelihood Estimation.&lt;/p&gt;

&lt;p&gt;It’s major feature is its independence. The only thing we care about when using it is the data.
That unfortunately is also one of its major weaknesses, as we’ll see in the subsequent posts about the Bayesian methods.&lt;/p&gt;

&lt;p&gt;Other benefits of using it are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it works with any kind of distribution&lt;/li&gt;
  &lt;li&gt;it works for multidimensional distributions&lt;/li&gt;
  &lt;li&gt;you can use any kind of optimization method, not necessarily the derivatives.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;algebraic-derivation-of-mle-for-bernoulli-distribution&quot;&gt;Algebraic derivation of MLE for Bernoulli distribution&lt;/h1&gt;

&lt;p&gt;I really like to dot all the i’s ;) 
So here’s my attempt at showing you how I derived the formula.&lt;/p&gt;

&lt;p&gt;Let’s start with the differentiation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{n}{k} \left( k\theta^{k-1}(1-\theta)^{n-k} + \theta^{n}(n-k)(1-\theta)^{n-k-1} (-1) \right) = 0&lt;/script&gt;

&lt;p&gt;Derivative of a product breaks down to a sum of derivatives. And the trailing &lt;script type=&quot;math/tex&quot;&gt;(-1)&lt;/script&gt; comes from taking the derrivative of a nested function &lt;script type=&quot;math/tex&quot;&gt;\frac{d(1 - \theta)^{n-k}}{d\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; are constants, we might get rid of the ominous binomial &lt;script type=&quot;math/tex&quot;&gt;\binom{n}{k}&lt;/script&gt;, by the property of dividing both sides of equation by it (zero divided by whatever remains a zero).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k} - \theta^{n}(n-k)(1-\theta)^{n-k-1} = 0&lt;/script&gt;

&lt;p&gt;The summation won’t allow us to use division as a way of getting rid of unwanted stuff that easily. So my first hunch is to rearrange the terms.
That way I can use the equals sign as a mirror and check if there are any symmetrical terms - terms that occur on both sides that I can &lt;em&gt;divide away&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k} = \theta^{k}(n-k)(1-\theta)^{n-k-1}&lt;/script&gt;

&lt;p&gt;The next transformation becomes immediately obvious.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta^{k}&lt;/script&gt; can be broken down into &lt;script type=&quot;math/tex&quot;&gt;\theta^{k-1} * \theta&lt;/script&gt; which gives us &lt;script type=&quot;math/tex&quot;&gt;\theta^{k-1}&lt;/script&gt; on both sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign. The same transformation can be applied to (1-\theta)^{n-k}&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k-1}(1-\theta) = \theta^{k-1}\theta(n-k)(1-\theta)^{n-k-1}&lt;/script&gt;

&lt;p&gt;Now we can divide them away, and we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k(1-\theta) = \theta(n-k)&lt;/script&gt;

&lt;p&gt;If we unroll the terms in the parenthesis, we get the term &lt;script type=&quot;math/tex&quot;&gt;-n\theta&lt;/script&gt; on boths sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k - k\theta = n\theta - k\theta&lt;/script&gt;

&lt;p&gt;We can get rid of &lt;script type=&quot;math/tex&quot;&gt;- k\theta&lt;/script&gt; which occurs on both sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k = n\theta&lt;/script&gt;

&lt;p&gt;And that leads straight to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \frac{k}{n}&lt;/script&gt;

&lt;h1 id=&quot;literature&quot;&gt;Literature&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&quot;&gt;Thinking fast and slow&lt;/a&gt;, Daniel Kahneman&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://books.google.co.uk/books?id=fCmpBwAAQBAJ&amp;amp;source=gbs_similarbooks&quot;&gt;Probability and statistics&lt;/a&gt;, Morris DeGroot, Mark Schervish&lt;/li&gt;
&lt;/ol&gt;</content><author><name>ptrochim</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Control your emotions</title><link href="http://localhost:4000/psychology/2019/05/19/anger.html" rel="alternate" type="text/html" title="Control your emotions" /><published>2019-05-19T00:00:00+01:00</published><updated>2019-05-19T00:00:00+01:00</updated><id>http://localhost:4000/psychology/2019/05/19/anger</id><content type="html" xml:base="http://localhost:4000/psychology/2019/05/19/anger.html">&lt;h1 id=&quot;emotions&quot;&gt;Emotions&lt;/h1&gt;

&lt;p&gt;Imagine waking up one sunny morning. You had a delicious breakfast, jumped in the car, and headed for that motorway. 
The traffic is heavy, cars queing to the motorway entrance. And as you’re sitting there, waiting patiently for your turn, you see a few cars cutting through in a pathetic attempt to ‘outsmart’ everyone else. Next thing you notice is a knot that ties inside your belly.&lt;/p&gt;

&lt;p&gt;Imagine a beautiful evening. You’re about to kick back with a game you’ve been looking forward to all day long, when you hear your wife calling for dinner. You shout that you’re not hungry and will come later, but she doesn’t yield. Soon you start shouting at each other.&lt;/p&gt;

&lt;p&gt;Imagine sitting at your office. Your boss has just called to a 1:1 meeting. You have no idea what it’s about, but as you approach the room, you feel your stomach churn. Turns out he wanted to congratulate you for a job well done on the last project.&lt;/p&gt;

&lt;p&gt;These seemingly casual situations ellicit very strong emotions that tend to spiral out of control. Seemingly different, they have one thing in common. It’s what happens &lt;em&gt;after&lt;/em&gt; the situation is over. It’s what happens when you leave it all behind and get to safety and solitude of your home/office/toilet…
It’s the ruminations, the anxiety, and ultimately the feeling of guilt for behaving like you did.&lt;/p&gt;

&lt;p&gt;It’s the depressing feeling that you could have handled it better, if only…&lt;/p&gt;

&lt;h1 id=&quot;what-are-they-for&quot;&gt;What are they for?&lt;/h1&gt;

&lt;p&gt;I’m not a stranger to those situations and those feelings.  But one sunny morning, after a particularily dark evening, I had enough.&lt;/p&gt;

&lt;p&gt;Every single time, I was left much worse off than before. 
Every single time, I wished I hadn’t done what I did. 
So how come, having experienced countless number of such situations, I was still walking into their trap each and every time?&lt;/p&gt;

&lt;p&gt;I heard someone saying that these are our primal fight-or-flight responses. That didn’t make any sense however. Shouldn’t those activate in life threatning situations? What’s so life threatning about a bunch of idiots cutting car lanes to get to the motorway faster?
One could argue that a boss calling you from out of the blue to a room spells something bad - but experience shows that more often than not it’s a positive or a neutral thing.&lt;/p&gt;

&lt;p&gt;So what is it that makes us - human - react the exact same way each and every time?&lt;/p&gt;

&lt;h1 id=&quot;what-affects-them&quot;&gt;What affects them&lt;/h1&gt;

&lt;p&gt;So I decided to get to the bottom of what makes me behave irrationally in perfectly rational situations.
Turns out there’s ample literature out there on the topic of anxiety, fear responses and how to handle them.&lt;/p&gt;

&lt;p&gt;I was surprised to find out how many things affected my emotional state:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;physical sensations, like hunger, thirst, pain and fatigue&lt;/li&gt;
  &lt;li&gt;being focused on something&lt;/li&gt;
  &lt;li&gt;prior emotional state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It has a lot to do with the Amygdalas, fear responses &lt;a href=&quot;#bibliography&quot;&gt;[1]&lt;/a&gt; and how our brain interprets non-verbal communication &lt;a href=&quot;#bibliography&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next, I put what I learned into practice. I experimented with many techniques, worked with a few therapists of different specialities, to find the combination that really works for me.&lt;/p&gt;

&lt;p&gt;In the end, I managed to construct a general mechanism that would allow me to approach everyday situations without emotion.&lt;/p&gt;

&lt;p&gt;It consists of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ability to observe one’s physical and emotional state&lt;/li&gt;
  &lt;li&gt;ability to understand and tend to one’s physical and emotional needs&lt;/li&gt;
  &lt;li&gt;ability to give assertive responses&lt;/li&gt;
  &lt;li&gt;ability to expand your comfort zone&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observe-my-needs&quot;&gt;Observe my needs&lt;/h2&gt;

&lt;p&gt;I learned that putting responsibilities over my physical and mental needs did more damage than good.&lt;/p&gt;

&lt;p&gt;So I learned how to notice and understand my physical and emotional sensations.&lt;/p&gt;

&lt;p&gt;I may decide to politely excuse myself out of a conversation, because I noticed that I was tired or too focused on something else.
I may decide to drink or eat something when I noticed that I couldn’t focus my attention on a task at hand.&lt;/p&gt;

&lt;p&gt;The technique I used to develop this ability was Mindfulness &lt;a href=&quot;#bibliography&quot;&gt;[2]&lt;/a&gt;.
All I needed was the &lt;a href=&quot;https://www.youtube.com/watch?v=rOne1P0TKL8&quot;&gt;3 minute breathing exercise&lt;/a&gt;, followed by focusing my attention on my body and how it felt (hungry, thirsty, tired).&lt;/p&gt;

&lt;h2 id=&quot;attend-to-them&quot;&gt;Attend to them&lt;/h2&gt;

&lt;p&gt;I learned that when I crave something, my focus drifts.
It’s quite obvious if you think about it. Recall what happens when you get very hungry.&lt;/p&gt;

&lt;p&gt;I found it to be true for other cravings too - the need for attention, rest, human contact, need to be understood.&lt;/p&gt;

&lt;p&gt;When I was growing up, I learned to supress those needs. I had perceived them as demeaning. 
So I had to re-learn how to accept and tend to them.
The happier I am in the body I inhabit, the happier I am being around others and facing challanges.&lt;/p&gt;

&lt;p&gt;The prerequisite to this step was the ability to observe and understand my own body. I worked with a psychotherapist of removing mental blockades that were supressing my needs. 
We used &lt;a href=&quot;https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy&quot;&gt;Cognitive Behavioral Therapy&lt;/a&gt;. The process took about 3 years.&lt;/p&gt;

&lt;h2 id=&quot;be-assertive-about-them&quot;&gt;Be assertive about them&lt;/h2&gt;

&lt;p&gt;I seldom had control over when an uncomfortable situation would befall me.&lt;/p&gt;

&lt;p&gt;In such cases I felt compelled to follow through, because otherwise I thought I would hurt someone else’s feelings. But I wasn’t putting my heart in what I was doing, and the effect was usually even more damaging.&lt;/p&gt;

&lt;p&gt;I worked with a psychotherapist to learn how to be polite but assertive.
A prerequisite to that was also the ability to observe my needs and heed to them.&lt;/p&gt;

&lt;h2 id=&quot;train-your-amygdalas&quot;&gt;Train your Amygdalas&lt;/h2&gt;

&lt;p&gt;The three techniques listed so far were a prerequisite that allowed me to control my emotions.
But every situation that had yielded them was yielding them still.&lt;/p&gt;

&lt;p&gt;So I learned how to calm my Amygdalas &lt;a href=&quot;#bibliography&quot;&gt;[3]&lt;/a&gt; when I felt uncomfortable. In what one might call “stepping out of one’s comfort zone”. And then, what used to scare me, started exciting me.&lt;/p&gt;

&lt;p&gt;To give a concrete example, chatting to complete strangers used to terrify me. Now I find it the most pleasant part of the day.&lt;/p&gt;

&lt;p&gt;The technique I used was a combination of breath control meditation I learned while studying Mindfulness &lt;a href=&quot;#bibliography&quot;&gt;[2]&lt;/a&gt;, followed by a simple visualization technique.&lt;/p&gt;

&lt;p&gt;Whenever an unpleasant event occured, I took a few deep breaths with my eyes closed, and I repeated a positive affirmation phrase (ie. “it’s ok, it’s not scary”). 
If the situation was really overwhelming, I seeked a safe place and launched into the meditation there. 
Then I actively seeked to repeat that experience and kept meditating through it, until I no longer responded emotionally.&lt;/p&gt;

&lt;p&gt;…I guess they are right when they say “get back on the horse that bucked you”.&lt;/p&gt;

&lt;h1 id=&quot;credits&quot;&gt;Credits&lt;/h1&gt;

&lt;p&gt;As I mentioned at the beginning, this wasn’t a lonely effort. 
I would like to thank each and every person I came across during this time. All of you had a huge impact on this investigation, and its ultimate success.&lt;/p&gt;

&lt;p&gt;My special thanks go to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Danusia, my wife - for all the love and support.&lt;/li&gt;
  &lt;li&gt;dr. Anna Mochnaczewska, my psychotherapist - for being a great and patient teacher.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you all from the bottom of my heart!&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;

&lt;p&gt;[1] “The role of the lateral amygdala in the retrieval and maintenance of fear-memories formed by repeated probabilistic reinforcement”, Jeffrey C. Erlich, David E. A. Bush, and Joseph E. LeDoux, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322351/&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322351/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Mindfulness, Wikipedia, &lt;a href=&quot;https://en.wikipedia.org/wiki/Mindfulness&quot;&gt;https://en.wikipedia.org/wiki/Mindfulness&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] “Rewire your Anxious Brain, how to use the neuroscience of fear to end anxiety, panic &amp;amp; worry”, Catherine M. Pittmann, Ph.D, Elizabeth M. Karle, MLIS; New Harbinger Publications Inc., 2015&lt;/p&gt;

&lt;p&gt;[4] “What Every Body is Saying”, Joe Navarro with Marcin Karlins, Ph.D., EPUB Edition, 2008&lt;/p&gt;</content><author><name>ptrochim</name></author><summary type="html">Emotions</summary></entry></feed>