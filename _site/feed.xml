<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-22T16:55:33+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Piotr Trochim’s blog</title><subtitle>Thoughts and results I wish to persist.
</subtitle><entry><title type="html">Bayesian Inference</title><link href="http://localhost:4000/math/2020/05/16/bayesian-inference.html" rel="alternate" type="text/html" title="Bayesian Inference" /><published>2020-05-16T00:00:00+01:00</published><updated>2020-05-16T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/05/16/bayesian-inference</id><content type="html" xml:base="http://localhost:4000/math/2020/05/16/bayesian-inference.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Maximum Likelihood Estimate&lt;/a&gt; method I covered in &lt;a href=&quot;https://paksas.github.io/math/2020/05/15/mle.html&quot;&gt;the previous post&lt;/a&gt;, had its flaws, one of which being its ignorance to hypothesis that may shed light on the collected data. The method I will describe now addresses that very issue.&lt;/p&gt;

&lt;h1 id=&quot;back-alley-clown&quot;&gt;Back alley clown&lt;/h1&gt;

&lt;p&gt;Let me recall the example from the previous post.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.&lt;/p&gt;

  &lt;p&gt;You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Back then though, all we cared about was the result of those 10 throws. But that’s not how humans work. We take circumstance into account.
I would be far more likely to respond to the challenge if I was attending a fair than if I was making a shortcut through a dark, dirty alley.&lt;/p&gt;

&lt;p&gt;I’m of course describing the process of &lt;em&gt;building confidence&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A fair clown is something rather desired, whereas one encounteres in a back alley would have to first gain our trust. His coin tosses would have to be pretty &lt;em&gt;convincing&lt;/em&gt;, showing no bias towards one side or another.&lt;/p&gt;

&lt;h1 id=&quot;two-distributions&quot;&gt;Two distributions&lt;/h1&gt;

&lt;p&gt;Mathematically speaking, we are still looking for the parameters of a distribution that models the Random Variable that is the clown’s coin.&lt;/p&gt;

&lt;p&gt;However now in addition to solid evidence, we also have our &lt;em&gt;prior&lt;/em&gt; beliefs. Those two combined allow us not only to make a guess about the parameter value, but also reflect our level of &lt;em&gt;confidence&lt;/em&gt; in that guess.&lt;/p&gt;

&lt;p&gt;If you recall, MLE direcly calculated the values of distribution parameters - it didn’t produce any value that would suggest a doubt about that value. It made sense, since the method relied exclusively on the data, which is a solid evidence. So unless there was other data available, the method was 100% confident about its estimate.&lt;/p&gt;

&lt;p&gt;Right now we want to express our &lt;em&gt;doubt&lt;/em&gt;, and &lt;em&gt;doubts&lt;/em&gt; are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to &lt;em&gt;sample&lt;/em&gt; the values of our &lt;em&gt;target distribution&lt;/em&gt; parameters.&lt;/p&gt;

&lt;p&gt;To reiterate:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLE -&amp;gt; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Bayesian Inference -&amp;gt; &lt;script type=&quot;math/tex&quot;&gt;\theta \sim P(\Theta)&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; sampled from a distribution &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; of the random variable &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayesian-inference&quot;&gt;Bayesian Inference&lt;/h1&gt;

&lt;p&gt;As a reminder, the task of any parameter inference is to find parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; based on some observed data &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.
In the specific case of Bayesian Inference, &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is estimated indirectly rather than being directly calculated.&lt;/p&gt;

&lt;p&gt;This estimation is achieved by introducing a random variable &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; that models the distribution of the parameter values.
Our task now is to calculate the values of &lt;em&gt;that&lt;/em&gt; distribution, rather than to directly calculate the values of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, and use that parameter to parameterize the &lt;em&gt;target distributon&lt;/em&gt; from which the data is sampled:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases} \theta \sim P(\Theta) \\ x \sim P(X | \Theta=\theta) \end{cases}&lt;/script&gt;

&lt;h1 id=&quot;bayes-theorem&quot;&gt;Bayes theorem&lt;/h1&gt;

&lt;p&gt;The method makes use of the famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes Theorem&lt;/a&gt;.
It’s quite ubiquitous, and there is a ton of good material out there explaining it in great detail. Therefore I’m going to assume that you’re vaguely familiar with it as well as its main use cases.
If I may recommend a book that had the largest impact on my understanding, it would be: &lt;a href=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s quickly recap the theorem and how can it lend itself ot our goal:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(B|A)*P(A)}{P(B)}&lt;/script&gt;

&lt;p&gt;Let’s introduce its elements and the terminology:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(A\|B)&lt;/script&gt; is called &lt;em&gt;the posterior&lt;/em&gt; and expresses our belief about &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; after observing &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(A)&lt;/script&gt; is called &lt;em&gt;the prior&lt;/em&gt; and expresses our belief about &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; before observing &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(B\|A)&lt;/script&gt; is called &lt;em&gt;the likelihood&lt;/em&gt; and expresses the way in which &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; affects our belief about &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(B)&lt;/script&gt; is sometimes called &lt;em&gt;the evidence&lt;/em&gt; and functions as a normalizing factor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reading this equation directly isn’t very informative. You immediately start asking the following questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if it’s &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; that we have observed (the data) and &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is what we want to estimate, then how do we calculate &lt;script type=&quot;math/tex&quot;&gt;P(B\|A)&lt;/script&gt;?&lt;/li&gt;
  &lt;li&gt;..in fact, why don’t we calculate &lt;script type=&quot;math/tex&quot;&gt;P(A\|B)&lt;/script&gt; directly using some distribution equation?&lt;/li&gt;
  &lt;li&gt;what is this normalizing factor &lt;script type=&quot;math/tex&quot;&gt;P(B)&lt;/script&gt; and how to calculate it?&lt;/li&gt;
  &lt;li&gt;what distribution equations to use for all those terms?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I want to focs on building thorough understanding of this before proceeding to the inference problem.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;h1 id=&quot;meaning-of-random-variables-a-and-b&quot;&gt;Meaning of random variables A and B&lt;/h1&gt;

&lt;p&gt;We want to estimate the values of parameters given some observed data, and the left hand side of Bayes theorem is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B)&lt;/script&gt;

&lt;p&gt;That would make:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; stand for the random variable that represents the parameters estimation &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; be the observed data &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{P(X)}&lt;/script&gt;

&lt;p&gt;Let’s recal our problem formulation. We are looking for a distribution parameterized by some value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, from which the data samples (coin tosses) are drawn - &lt;script type=&quot;math/tex&quot;&gt;x \sim P(\theta)&lt;/script&gt;.
But instead of calculating the value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; directly, we want to express our belief about its value, so we choose to model it with a random variable &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;.
We are then going to use that random variable to parameterize the distribution we draw our data from:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases} \theta \sim P(\Theta) \\ x \sim P(X | \Theta=\theta) \end{cases}&lt;/script&gt;

&lt;p&gt;Here, lowercase &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; denote the samples, and their uppercase equivalents &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; - random variables.
Notice that &lt;script type=&quot;math/tex&quot;&gt;P(X | \Theta)&lt;/script&gt; appears in the Bayesian equation, however not on its left hand side. It is in fact the likelihood.&lt;/p&gt;

&lt;p&gt;This is very confusing. Are we calculating the wrong value? Should our equation look like this then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X | \Theta) = \frac{P(\Theta | X)*P(X)}{P(\Theta)}&lt;/script&gt;

&lt;p&gt;The answer is NO. We are looking for P(\Theta), but the one that takes into account the observed data &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; - &lt;script type=&quot;math/tex&quot;&gt;P(\Theta | X)&lt;/script&gt;.
It is that distribution that will express our belief about the parameterization of the data distribution.&lt;/p&gt;

&lt;p&gt;But then how do we write down the &lt;em&gt;target data distribution&lt;/em&gt;? We have several options, all of which definitely need to take &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; into account as well as the random variable &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, which models our data.
We know for sure that it’s NOT &lt;script type=&quot;math/tex&quot;&gt;P(\Theta)&lt;/script&gt; - that’s the distribution of the parameters. So is it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X, \Theta)&lt;/script&gt; ?&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X \| \Theta)&lt;/script&gt; ?&lt;/li&gt;
  &lt;li&gt;or perhaps &lt;script type=&quot;math/tex&quot;&gt;P(X)&lt;/script&gt; with an &lt;em&gt;invisible&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; parameterization everyone except for me sort of knows about?&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;distribution-function-notation&quot;&gt;Distribution function notation&lt;/h1&gt;

&lt;p&gt;Let’s see what different notations mean visually. To get nice visual results, I used a Normal distribution &lt;script type=&quot;math/tex&quot;&gt;N(\mu=0, \sigma=1)&lt;/script&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Notation&lt;/th&gt;
      &lt;th&gt;Visual representation&lt;/th&gt;
      &lt;th&gt;Comment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_1.png&quot; alt=&quot;Fig 1&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;Probability density function&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X=x)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_2.png&quot; alt=&quot;Fig 2&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;Probability density value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X, Y)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_3.png&quot; alt=&quot;Fig 3&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;Joint (2D) probability density function)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X=x, Y=y)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_4.png&quot; alt=&quot;Fig 4&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;Probability density value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X, Y=y)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P(X | Y=y)&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_5.png&quot; alt=&quot;Fig 5&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;A slice through a 2D function&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notice how the conditional probability slices through the data. We could as well use the notation &lt;script type=&quot;math/tex&quot;&gt;P(X, Y=y)&lt;/script&gt;, but the vertical bar notation was adopted for this purpose, and we call this slice “conditional” probability to emphasize that we are fixing the value of one of the variables.&lt;/p&gt;

&lt;p&gt;Let’s see which type of notation best describe the &lt;em&gt;target distribution&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X, \Theta)&lt;/script&gt; - both the data &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and the parameters &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; haven’t been sampled, and we are talking about a joint function of both&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X \| \Theta)&lt;/script&gt; - we have observed some value of &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; and we want to use it to take a slice from &lt;script type=&quot;math/tex&quot;&gt;P(X, \Theta)&lt;/script&gt;. We will consider this slice a 1D distribution of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\Theta \| X)&lt;/script&gt; - we have observed some value of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and we want to use it to take a slice from &lt;script type=&quot;math/tex&quot;&gt;P(X, \Theta)&lt;/script&gt;. We will consider this slice a 1D distribution of &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s clearly the 2nd one - &lt;script type=&quot;math/tex&quot;&gt;P(X \| \Theta)&lt;/script&gt;. It feels like we’ve gone full circle - the likelihood term of the Bayes Theorem is denoted in the same way as the &lt;em&gt;target distribution&lt;/em&gt; we will be sampling from.&lt;/p&gt;

&lt;p&gt;This confused me for a very long time, and it stemmed from how nuanced the probability notation really is. Let’s looks at the equations side by side:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Bayes Theorem&lt;/th&gt;
      &lt;th&gt;Model of data generator&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{P(X)}&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;x \sim P(X|\Theta)&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I searched long and hard for an explanation and, to my great disappointment, I didn’t find any. But I did find out a few things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P(X\|\Theta)&lt;/script&gt; in both cases can be read as &lt;em&gt;“conditional distribution of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;In Bayes Theorem cast for finding the relationship between conditional distributions, that’s also what it is&lt;/li&gt;
  &lt;li&gt;in Bayes Theorem cast for the inference problems, that conditional distribution isn’t a distribution at all - if you would integrate the area under its curve, that area wouldn’t equal 1.0.&lt;/li&gt;
  &lt;li&gt;In out model of data generator, it should be read as the conditional distribution, but it’s a completely different instance of the distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It seems that the confusion stems from the fact that &lt;script type=&quot;math/tex&quot;&gt;P(X\|\Theta)&lt;/script&gt; in the Bayesian Inference equation - the so-called &lt;em&gt;likelihood&lt;/em&gt; term - isn’t a distribution at all. What is it then and why is it using the same notation as a distribution?&lt;/p&gt;

&lt;h1 id=&quot;likelihood-function&quot;&gt;Likelihood function&lt;/h1&gt;

&lt;p&gt;As we’ll see a bit later, the likelihood term is modeled using the equations of actual distribution functions, but it isn’t a distribution. The reason is that we are actually plugging in ALL of the values.
It should be written as &lt;script type=&quot;math/tex&quot;&gt;P(X=x|\Theta=\theta)&lt;/script&gt; rather than &lt;script type=&quot;math/tex&quot;&gt;P(X|\Theta=\theta)&lt;/script&gt;, which suggests &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; being a random variable and us modelling some sort of a distribution shape with it.&lt;/p&gt;

&lt;p&gt;This function returns 2 different things, depending on the type of distribution function equation used:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Distribution function type&lt;/th&gt;
      &lt;th&gt;Likelihood return value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;discrete&lt;/td&gt;
      &lt;td&gt;probability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;continuous&lt;/td&gt;
      &lt;td&gt;density&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Probability density, because that’s the density I’m referring to, isn’t equivalent to the probability. But in case of a regular probability density function, should you integrate the area under the entire curve, it would equal to 1.0.&lt;/p&gt;

&lt;p&gt;So why is it that in the case of likelihood it wouldn’t?&lt;/p&gt;

&lt;h1 id=&quot;the-parameters-of-the-likelihood-function&quot;&gt;The parameters of the likelihood function&lt;/h1&gt;

&lt;p&gt;The answer lies in the function support - another nuance of the notation we usually don’t bother to focus on at all.&lt;/p&gt;

&lt;p&gt;As I mentioned before, the likelihood term is expressed using a regular distribution function equation. I want to use a concrete example to drive the point here, and since we’re talking about coin tosses, I’m going to use the Binomial distribution equation. But please keep in mind that this applies to ANY distribution just the same.&lt;/p&gt;

&lt;p&gt;The equation of Binomial distribution probability &lt;em&gt;mass&lt;/em&gt; function: &lt;script type=&quot;math/tex&quot;&gt;f(k, n, p) = \binom{n}{k} p^{k}(1-p)^{n-k}&lt;/script&gt;. It’s a discrete distribution.
It’s discreteness stems from the fact that its support is a subset of integers defined as &lt;script type=&quot;math/tex&quot;&gt;k \in {0..n}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;One would use this equation to express a distribution of probability by choosing some value of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, and then iterating over all possible values of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and running the three through this equation.
Let’s say that we want to model the distribution of getting k Heads in 10 tosses, and the tosses are performed with a fair coin (p=0.5).
The equation would then take the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(10, k, 0.5) = \binom{10}{k} 0.5^{k}(1-0.5)^{10-k}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_6.png&quot; alt=&quot;Fig 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you would integrate (sum up in this case) the values, the result would equal 1.0.&lt;/p&gt;

&lt;p&gt;But &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; isn’t the only parameter we can iterate over. What would happen if we fixed it, and iterated over all possible values of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; instead?
To be more specific - we have performed 10 tosses, 3 of them were Heads. How does the probability change for different values of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; ?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(10, 3, p) = \binom{10}{3} p^{3}(1-p)^{10-3} = 120 p^{3}(1-p)^{7}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_7.png&quot; alt=&quot;Fig 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice a few things off the bat:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the plot is no longer choppy.&lt;/li&gt;
  &lt;li&gt;it has a different range - (0, 1) rather than {0..10}&lt;/li&gt;
  &lt;li&gt;the integral of this function equals 0.09, instead of 1, which should be the case if this was a proper distribution function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bayesian Inference writes the likelihood as if it was something that can be modelled as a joint distribution of two variables - the data and the parameterization. Specifically:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(n, k, p) = P(X \| \Theta)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;X: k&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Theta: (n, p)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The only justification I can think of at the time of writing this article is that this is a notation abuse.
As we’ve seen in the case of Binomial distribution function (and other functions follow suite), there is a very specific support defined and one can’t just turn other parameters into random variables and expect to obtain a joint distributon.&lt;/p&gt;

&lt;p&gt;In addition to causing confusion, it also introduces mathematical discrepancy - the nominator can no longer be treated as a distribution. So in order to bring the posterior back on track to being a regular distribution, another term needs to step in and act as a normalizer.&lt;/p&gt;

&lt;h1 id=&quot;the-normalizing-role-of-the-evidence-term&quot;&gt;The normalizing role of the evidence term.&lt;/h1&gt;

&lt;p&gt;Going back to the Bayes Theorem, we find the evidence defined as &lt;script type=&quot;math/tex&quot;&gt;P(X)&lt;/script&gt;.
The extended version of the Theorem rewrites this term as &lt;script type=&quot;math/tex&quot;&gt;P(X) = \int P(X | \Theta=\theta) d\theta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{\int P(X | \Theta=\theta) d\theta}&lt;/script&gt;

&lt;p&gt;It means that we will marginalize random variable &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, which represents the model of our data, for all possible values of &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;. That marginalization would guarantee a distribution, if &lt;script type=&quot;math/tex&quot;&gt;P(X\|\Theta)&lt;/script&gt; was in fact a distribution, which it isn’t.&lt;/p&gt;

&lt;p&gt;However - it will normalize the nominator such that the calculated posterior P(\Theta | X) will be a distribution! This does not depend on the type of distribution equations used.
I would however appreciate a proof, so if anyone knows it, please share.&lt;/p&gt;

&lt;h1 id=&quot;can-maximum-likelihood-estimate-be-used-to-calculate-the-likelihood-term&quot;&gt;Can Maximum Likelihood Estimate be used to calculate the Likelihood term?&lt;/h1&gt;

&lt;p&gt;To close the topic of confusing notation, I wanted to bring up a strange similarity of names between the Maximum Likelihood Estimate and the Likelihood term in the Bayesian Inference.
What confused me in particular was whether MLE &lt;em&gt;should&lt;/em&gt; be used to calculate the value returned by the likelihood term.&lt;/p&gt;

&lt;p&gt;There is no relation between the two:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As we’ve learned, the likelihood term returns some value or function, which shouldn’t be treated as a probability distribution.&lt;/li&gt;
  &lt;li&gt;The Maximum Likelihood Estimate is a method to assess the value of distribution parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two return completely different things. In fact, on the conceptual level, MLE produces the same values that sampling from the posterior returned by Bayesian Inference does:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases} \theta \sim MLE(X) \\ \theta \sim P(\Theta | X) \end{cases}&lt;/script&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work&lt;/h2&gt;

&lt;p&gt;Now that we know how each part of the equation works, I wanted to show how it can be used.&lt;/p&gt;

&lt;p&gt;Allow me to pull some fake data out of the hat and use it to solve our coin tossing problem.
For example - let’s say that I’ve observed the clown tossing 7 Heads in 10 coin tosses.
As a reminder, I now need to call the result of the next toss. In order to do that, I need to estimate the “fairness” of the coin, or our parameter &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; that we have seen used in the Binomial distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(n, k, p) = \binom{n}{k}p^k(1-p)^{n-k}&lt;/script&gt;

&lt;p&gt;Once I know the value of that parameter, I will be able to determine which side is more likely to show up on the next toss. If the value is closer to 1 - it will be the Heads, if it’s closer to 0 - Tails.
The value of 0.5 will put me in a pickle, because it would mean the coin is fair and I would have to guess.&lt;/p&gt;

&lt;p&gt;Maximum Likelihood Estimate would suggest the coin is biased towards Heads. However, I have some other information I can incorporate in my assessment.&lt;/p&gt;

&lt;h1 id=&quot;choosing-a-prior&quot;&gt;Choosing a prior&lt;/h1&gt;

&lt;p&gt;My first clue is that I encountered the clown while taking a shortcut through a dark back alley. Yes, this isn’t a fair, or any other place where you would expect to meet a coin tossing clown.
I suspect that he will try to trick me and he’s somehow controlling the coin.&lt;/p&gt;

&lt;p&gt;My suspicion is that it’s biased towards Tails.&lt;/p&gt;

&lt;p&gt;I will use a Beta distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha - 1}(1-\theta)^{\beta - 1}&lt;/script&gt;

&lt;p&gt;with parameters &lt;script type=&quot;math/tex&quot;&gt;alpha=1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;beta=3&lt;/script&gt; as a prior, which I think best reflects that belief:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta) = \frac{\Gamma(4)}{\Gamma(1)\Gamma(3)}\theta^{0}(1-\theta)^{2} = 3 (1-\theta)^{2}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_8.png&quot; alt=&quot;Fig 8&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;choosing-the-likelihood-function&quot;&gt;Choosing the likelihood function&lt;/h1&gt;

&lt;p&gt;The likelihood function needs to appraise the collected data and return a &lt;em&gt;metric&lt;/em&gt; (not a probability, but a metric), how likely that data was given the hypothesized parameter values.
The choice of the function is arbitrary, and often dictated by a similarity to a particular distribution model.&lt;/p&gt;

&lt;p&gt;That is also why I chose to use the Binomial Distribution function with fixed parameters &lt;script type=&quot;math/tex&quot;&gt;(n, k)&lt;/script&gt; as the likelihood function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X\|\Theta) = \binom{10}{7}\theta^{7}(1-\theta)^3=120[\theta^{7}(1-\theta)^3]&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_9.png&quot; alt=&quot;Fig 9&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;calculating-the-evidence-denominator&quot;&gt;Calculating the evidence denominator&lt;/h1&gt;

&lt;p&gt;Let’s start with the integral version of the Bayes Theorem denominator. &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; here has support in range (0, 1), which we will use as the integration limits.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X) = \int_{0}^{1} P(X\|\Theta) * P(\Theta) d\theta&lt;/script&gt;

&lt;p&gt;Substitute the functions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X) = \int_{0}^{1} [3(1-\theta)^{2}] [120[\theta^{7}(1-\theta)^3]] d\theta&lt;/script&gt;

&lt;p&gt;Extract the constants and combine the variables, and then use Wolfram Alpha to calculate the final result:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X) = 360 \int_{0}^{1} \theta^{7}(1-\theta)^{5} d\theta = \frac{5}{143}&lt;/script&gt;

&lt;p&gt;Notice that this result is a constant. That’s the normalizing constant I was referring to.&lt;/p&gt;

&lt;h1 id=&quot;put-it-all-together&quot;&gt;Put it all together&lt;/h1&gt;

&lt;p&gt;We can now substitute those equations into the Bayes Theorem equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta \| X) = \frac{P(X|\Theta)P(\Theta)}{P(X)} = \frac{[120[\theta^{7}(1-\theta)^3]][3 (1-\theta)^{2}]}{\frac{5}{143}}&lt;/script&gt;

&lt;p&gt;After combining the terms, we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Theta \| X) = 10296 \theta^{7}(1-\theta)^5&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_10.png&quot; alt=&quot;Fig 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And just to verify that it actually represents a probability distribution, its integral in &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; support equals 1:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{0}^{1} 10296 \theta^{7}(1-\theta)^5 = 1&lt;/script&gt;

&lt;h1 id=&quot;conjugate-priors&quot;&gt;Conjugate priors&lt;/h1&gt;

&lt;p&gt;Explain that in order to be able to use posterior as the next iteration prior, MLE and prior need to be conjugate distributions
Quote Pearse stats book on the conjugate prior
Beta is a conjugate of the binomial distr.&lt;/p&gt;</content><author><name>ptrochim</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Maximum Likelihood Estimate</title><link href="http://localhost:4000/math/2020/05/15/mle.html" rel="alternate" type="text/html" title="Maximum Likelihood Estimate" /><published>2020-05-15T00:00:00+01:00</published><updated>2020-05-15T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/05/15/mle</id><content type="html" xml:base="http://localhost:4000/math/2020/05/15/mle.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.&lt;/p&gt;

&lt;p&gt;In this post, I will be covering a method that relies exclusively on the data and doesn’t require the user to use any auxiliary distributions - Maximum Likelihood Estimate.&lt;/p&gt;

&lt;p&gt;I will assume that you are familiar with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The concept of a random variable&lt;/li&gt;
  &lt;li&gt;Binomial distribution&lt;/li&gt;
  &lt;li&gt;Expected value of a random variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;is-this-a-fair-coin&quot;&gt;Is this a fair coin?&lt;/h1&gt;

&lt;p&gt;Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.&lt;/p&gt;

&lt;p&gt;You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.&lt;/p&gt;

&lt;h1 id=&quot;what-you-see-is-all-there-is-wysiati&quot;&gt;What You See Is All There Is (WYSIATI)&lt;/h1&gt;

&lt;p&gt;This famous mnemonic coined by Daniel Kahneman in his book &lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&quot;&gt;“Thinking Fast and Slow”&lt;/a&gt; gives a hint how to proceed.&lt;/p&gt;

&lt;p&gt;What would be your guess if you have observed 10 Heads, and you had no other information to go on?&lt;/p&gt;

&lt;p&gt;If you did’t see the greedy smirk on the clown’s face, nor the long face of the person walking away from the table. You only saw the clown toss 10 Heads in a row.
What would be the result of the next one?&lt;/p&gt;

&lt;p&gt;Clearly, it’d be Heads. And if he threw 10 Tails, you would be stupid not to have bet on Tails… would you?&lt;/p&gt;

&lt;p&gt;As these thoughts run through your head, you are trying to guess a certain property - what to &lt;em&gt;expect&lt;/em&gt; from the coin as you keep tossing it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;10 Heads - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 1&lt;/script&gt;, so bet Heads&lt;/li&gt;
  &lt;li&gt;10 Tails - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 0&lt;/script&gt;, so bet Tails&lt;/li&gt;
  &lt;li&gt;5 Heads and 5 Tails - &lt;script type=&quot;math/tex&quot;&gt;E[X] = 0.5&lt;/script&gt;, so… make a guess :/&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We need to guess the &lt;em&gt;expected value&lt;/em&gt; of the Random Variable, otherwise known as the mean of its distribution.&lt;/p&gt;

&lt;h1 id=&quot;relating-data-to-a-random-variable&quot;&gt;Relating data to a Random Variable&lt;/h1&gt;

&lt;p&gt;Our coin tossing experiment is best represented by a Random Variable with a Binomial Distribution.&lt;/p&gt;

&lt;p&gt;A discrete Random Variable is a sampler that pulls items out of a set. How frequently will certain values show up is dictated by its probability (mass) function. And it’s expected value (or mean) indicates which particular item we can expect to see pulled out the most.&lt;/p&gt;

&lt;p&gt;But how do we reverse it? How do we guess the mean and parameterize the probability (mass) function given the data that has been pulled out?&lt;/p&gt;

&lt;h1 id=&quot;binomial-distribution-for-a-coin-tossing-problem&quot;&gt;Binomial distribution for a coin tossing problem&lt;/h1&gt;

&lt;p&gt;The expected value of R.V. (a.k.a the mean of its distribution) is a maxima of that R.V’s probability (mass) function.
The best way to find the maxima of a function is to solve its derivative for its roots !&lt;/p&gt;

&lt;p&gt;So let’s remind ourselves of Binomial distribution p.m.f:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\theta, n, k) = \binom{n}{k} \theta^{k} (1 - \theta)^{n-k}&lt;/script&gt;

&lt;p&gt;Where, in our case:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; - total number of coin tosses&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; - number of &lt;em&gt;positive&lt;/em&gt; tosses (in our case let’s assume those are represented by Heads)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; - a latent parameter that can be thought of as the fairness of our coin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Binomial distribution is a discrete distribution. The word discrete suggests something that’s not continuous, something we wouldn’t be able to differentiate, right?&lt;/p&gt;

&lt;h1 id=&quot;misleading-vocabulary&quot;&gt;Misleading vocabulary&lt;/h1&gt;

&lt;p&gt;When I think about a distribution, I think about its probability function.
If its support is a discrete domain, then it’s a discrete distribution. If it’s continuous - like a real numbers line for example - then the distribution is continuous.&lt;/p&gt;

&lt;p&gt;When I look at the probability mass function (they make you use the term mass to highlight the discretness of the distribution), I see three parameters:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Values&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;n \in {0, 1, ...}&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Discrete&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;k \in {0, 1, ..., n}&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Discrete&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;theta \in R&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;Continuous&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We clearly deal with two discrete parameters, and a continuous one. So which is it then?&lt;/p&gt;

&lt;p&gt;It depends on the use case actually. It also depends on the distribution (some, line the Normal distribution, have only continuous params).
But the bottom line is that you can change the support of the function. In our case:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if you were interested in finding a probability of a series of coin tosses given a specific coin, identified by a specific &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, you would fix its value, and the support would become fully discrete.&lt;/li&gt;
  &lt;li&gt;if you were told what the results of the tosses were, and asked to guess the property of the coin, its &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, you would fix &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, and the continuous &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; would become its support.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Confusing, isn’t it. One function, so many different options and use cases.&lt;/p&gt;

&lt;p&gt;In our case however, these two parameters are fixed - we are given 10 observations (&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;), and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; of them are Heads. What we need to find is such value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; that will maximize our observations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;argmax f(\theta, n, k)&lt;/script&gt;

&lt;h1 id=&quot;solving-the-derivative&quot;&gt;Solving the derivative&lt;/h1&gt;

&lt;p&gt;In order to find the maxima, we’ll use the differential calculus - a battle hardened method for optimizing a function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(\theta, n, k)}{d\theta} = 0&lt;/script&gt;

&lt;p&gt;After &lt;a href=&quot;#algebraic-derivation-of-mle-for-bernoulli-distribution&quot;&gt;a little bit of algebra&lt;/a&gt;, we get &lt;script type=&quot;math/tex&quot;&gt;\theta = \frac{k}{n}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The result suggests that the parameter of Binomial Distribution’s probability mass function that maximizes the likelihood of observing clown’s coin toss results is… the ratio of obtained Heads &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; to the number of performed throws &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/p&gt;

&lt;h1 id=&quot;deeper-insight---the-average&quot;&gt;Deeper insight - the average&lt;/h1&gt;

&lt;p&gt;If we assumed that Tails are represented by 0, and Heads by a 1, and an &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; would represent &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; coin toss result, then &lt;script type=&quot;math/tex&quot;&gt;n = \sum_{i=0}^{n} x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Our result then becomes the &lt;em&gt;average&lt;/em&gt; value of the observed data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \frac{n}{k} = \frac{1}{k} \sum_{i=0}^{n} x_i&lt;/script&gt;

&lt;h1 id=&quot;maximum-likelihood-estimate&quot;&gt;Maximum Likelihood Estimate&lt;/h1&gt;

&lt;p&gt;The method the relies on finding the maxima of a probability mass/density function is called Maximum Likelihood Estimation.&lt;/p&gt;

&lt;p&gt;It’s major feature is its independence. The only thing we care about when using it is the data.
That unfortunately is also one of its major weaknesses, as we’ll see in the subsequent posts about the Bayesian methods.&lt;/p&gt;

&lt;p&gt;Other benefits of using it are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it works with any kind of distribution&lt;/li&gt;
  &lt;li&gt;it works for multidimensional distributions&lt;/li&gt;
  &lt;li&gt;you can use any kind of optimization method, not necessarily the derivatives.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;algebraic-derivation-of-mle-for-bernoulli-distribution&quot;&gt;Algebraic derivation of MLE for Bernoulli distribution&lt;/h1&gt;

&lt;p&gt;I really like to dot all the i’s ;) 
So here’s my attempt at showing you how I derived the formula.&lt;/p&gt;

&lt;p&gt;Let’s start with the differentiation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{n}{k} \left( k\theta^{k-1}(1-\theta)^{n-k} + \theta^{n}(n-k)(1-\theta)^{n-k-1} (-1) \right) = 0&lt;/script&gt;

&lt;p&gt;Derivative of a product breaks down to a sum of derivatives. And the trailing &lt;script type=&quot;math/tex&quot;&gt;(-1)&lt;/script&gt; comes from taking the derrivative of a nested function &lt;script type=&quot;math/tex&quot;&gt;\frac{d(1 - \theta)^{n-k}}{d\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; are constants, we might get rid of the ominous binomial &lt;script type=&quot;math/tex&quot;&gt;\binom{n}{k}&lt;/script&gt;, by the property of dividing both sides of equation by it (zero divided by whatever remains a zero).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k} - \theta^{n}(n-k)(1-\theta)^{n-k-1} = 0&lt;/script&gt;

&lt;p&gt;The summation won’t allow us to use division as a way of getting rid of unwanted stuff that easily. So my first hunch is to rearrange the terms.
That way I can use the equals sign as a mirror and check if there are any symmetrical terms - terms that occur on both sides that I can &lt;em&gt;divide away&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k} = \theta^{k}(n-k)(1-\theta)^{n-k-1}&lt;/script&gt;

&lt;p&gt;The next transformation becomes immediately obvious.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta^{k}&lt;/script&gt; can be broken down into &lt;script type=&quot;math/tex&quot;&gt;\theta^{k-1} * \theta&lt;/script&gt; which gives us &lt;script type=&quot;math/tex&quot;&gt;\theta^{k-1}&lt;/script&gt; on both sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign. The same transformation can be applied to (1-\theta)^{n-k}&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\theta^{k-1}(1-\theta)^{n-k-1}(1-\theta) = \theta^{k-1}\theta(n-k)(1-\theta)^{n-k-1}&lt;/script&gt;

&lt;p&gt;Now we can divide them away, and we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k(1-\theta) = \theta(n-k)&lt;/script&gt;

&lt;p&gt;If we unroll the terms in the parenthesis, we get the term &lt;script type=&quot;math/tex&quot;&gt;-n\theta&lt;/script&gt; on boths sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k - k\theta = n\theta - k\theta&lt;/script&gt;

&lt;p&gt;We can get rid of &lt;script type=&quot;math/tex&quot;&gt;- k\theta&lt;/script&gt; which occurs on both sides of the &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; sign:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k = n\theta&lt;/script&gt;

&lt;p&gt;And that leads straight to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \frac{k}{n}&lt;/script&gt;

&lt;h1 id=&quot;literature&quot;&gt;Literature&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&quot;&gt;Thinking fast and slow&lt;/a&gt;, Daniel Kahneman&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://books.google.co.uk/books?id=fCmpBwAAQBAJ&amp;amp;source=gbs_similarbooks&quot;&gt;Probability and statistics&lt;/a&gt;, Morris DeGroot, Mark Schervish&lt;/li&gt;
&lt;/ol&gt;</content><author><name>ptrochim</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Control your emotions</title><link href="http://localhost:4000/psychology/2019/05/19/anger.html" rel="alternate" type="text/html" title="Control your emotions" /><published>2019-05-19T00:00:00+01:00</published><updated>2019-05-19T00:00:00+01:00</updated><id>http://localhost:4000/psychology/2019/05/19/anger</id><content type="html" xml:base="http://localhost:4000/psychology/2019/05/19/anger.html">&lt;h1 id=&quot;emotions&quot;&gt;Emotions&lt;/h1&gt;

&lt;p&gt;Imagine waking up one sunny morning. You had a delicious breakfast, jumped in the car, and headed for that motorway. 
The traffic is heavy, cars queing to the motorway entrance. And as you’re sitting there, waiting patiently for your turn, you see a few cars cutting through in a pathetic attempt to ‘outsmart’ everyone else. Next thing you notice is a knot that ties inside your belly.&lt;/p&gt;

&lt;p&gt;Imagine a beautiful evening. You’re about to kick back with a game you’ve been looking forward to all day long, when you hear your wife calling for dinner. You shout that you’re not hungry and will come later, but she doesn’t yield. Soon you start shouting at each other.&lt;/p&gt;

&lt;p&gt;Imagine sitting at your office. Your boss has just called to a 1:1 meeting. You have no idea what it’s about, but as you approach the room, you feel your stomach churn. Turns out he wanted to congratulate you for a job well done on the last project.&lt;/p&gt;

&lt;p&gt;These seemingly casual situations ellicit very strong emotions that tend to spiral out of control. Seemingly different, they have one thing in common. It’s what happens &lt;em&gt;after&lt;/em&gt; the situation is over. It’s what happens when you leave it all behind and get to safety and solitude of your home/office/toilet…
It’s the ruminations, the anxiety, and ultimately the feeling of guilt for behaving like you did.&lt;/p&gt;

&lt;p&gt;It’s the depressing feeling that you could have handled it better, if only…&lt;/p&gt;

&lt;h1 id=&quot;what-are-they-for&quot;&gt;What are they for?&lt;/h1&gt;

&lt;p&gt;I’m not a stranger to those situations and those feelings.  But one sunny morning, after a particularily dark evening, I had enough.&lt;/p&gt;

&lt;p&gt;Every single time, I was left much worse off than before. 
Every single time, I wished I hadn’t done what I did. 
So how come, having experienced countless number of such situations, I was still walking into their trap each and every time?&lt;/p&gt;

&lt;p&gt;I heard someone saying that these are our primal fight-or-flight responses. That didn’t make any sense however. Shouldn’t those activate in life threatning situations? What’s so life threatning about a bunch of idiots cutting car lanes to get to the motorway faster?
One could argue that a boss calling you from out of the blue to a room spells something bad - but experience shows that more often than not it’s a positive or a neutral thing.&lt;/p&gt;

&lt;p&gt;So what is it that makes us - human - react the exact same way each and every time?&lt;/p&gt;

&lt;h1 id=&quot;what-affects-them&quot;&gt;What affects them&lt;/h1&gt;

&lt;p&gt;So I decided to get to the bottom of what makes me behave irrationally in perfectly rational situations.
Turns out there’s ample literature out there on the topic of anxiety, fear responses and how to handle them.&lt;/p&gt;

&lt;p&gt;I was surprised to find out how many things affected my emotional state:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;physical sensations, like hunger, thirst, pain and fatigue&lt;/li&gt;
  &lt;li&gt;being focused on something&lt;/li&gt;
  &lt;li&gt;prior emotional state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It has a lot to do with the Amygdalas, fear responses &lt;a href=&quot;#bibliography&quot;&gt;[1]&lt;/a&gt; and how our brain interprets non-verbal communication &lt;a href=&quot;#bibliography&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next, I put what I learned into practice. I experimented with many techniques, worked with a few therapists of different specialities, to find the combination that really works for me.&lt;/p&gt;

&lt;p&gt;In the end, I managed to construct a general mechanism that would allow me to approach everyday situations without emotion.&lt;/p&gt;

&lt;p&gt;It consists of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ability to observe one’s physical and emotional state&lt;/li&gt;
  &lt;li&gt;ability to understand and tend to one’s physical and emotional needs&lt;/li&gt;
  &lt;li&gt;ability to give assertive responses&lt;/li&gt;
  &lt;li&gt;ability to expand your comfort zone&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observe-my-needs&quot;&gt;Observe my needs&lt;/h2&gt;

&lt;p&gt;I learned that putting responsibilities over my physical and mental needs did more damage than good.&lt;/p&gt;

&lt;p&gt;So I learned how to notice and understand my physical and emotional sensations.&lt;/p&gt;

&lt;p&gt;I may decide to politely excuse myself out of a conversation, because I noticed that I was tired or too focused on something else.
I may decide to drink or eat something when I noticed that I couldn’t focus my attention on a task at hand.&lt;/p&gt;

&lt;p&gt;The technique I used to develop this ability was Mindfulness &lt;a href=&quot;#bibliography&quot;&gt;[2]&lt;/a&gt;.
All I needed was the &lt;a href=&quot;https://www.youtube.com/watch?v=rOne1P0TKL8&quot;&gt;3 minute breathing exercise&lt;/a&gt;, followed by focusing my attention on my body and how it felt (hungry, thirsty, tired).&lt;/p&gt;

&lt;h2 id=&quot;attend-to-them&quot;&gt;Attend to them&lt;/h2&gt;

&lt;p&gt;I learned that when I crave something, my focus drifts.
It’s quite obvious if you think about it. Recall what happens when you get very hungry.&lt;/p&gt;

&lt;p&gt;I found it to be true for other cravings too - the need for attention, rest, human contact, need to be understood.&lt;/p&gt;

&lt;p&gt;When I was growing up, I learned to supress those needs. I had perceived them as demeaning. 
So I had to re-learn how to accept and tend to them.
The happier I am in the body I inhabit, the happier I am being around others and facing challanges.&lt;/p&gt;

&lt;p&gt;The prerequisite to this step was the ability to observe and understand my own body. I worked with a psychotherapist of removing mental blockades that were supressing my needs. 
We used &lt;a href=&quot;https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy&quot;&gt;Cognitive Behavioral Therapy&lt;/a&gt;. The process took about 3 years.&lt;/p&gt;

&lt;h2 id=&quot;be-assertive-about-them&quot;&gt;Be assertive about them&lt;/h2&gt;

&lt;p&gt;I seldom had control over when an uncomfortable situation would befall me.&lt;/p&gt;

&lt;p&gt;In such cases I felt compelled to follow through, because otherwise I thought I would hurt someone else’s feelings. But I wasn’t putting my heart in what I was doing, and the effect was usually even more damaging.&lt;/p&gt;

&lt;p&gt;I worked with a psychotherapist to learn how to be polite but assertive.
A prerequisite to that was also the ability to observe my needs and heed to them.&lt;/p&gt;

&lt;h2 id=&quot;train-your-amygdalas&quot;&gt;Train your Amygdalas&lt;/h2&gt;

&lt;p&gt;The three techniques listed so far were a prerequisite that allowed me to control my emotions.
But every situation that had yielded them was yielding them still.&lt;/p&gt;

&lt;p&gt;So I learned how to calm my Amygdalas &lt;a href=&quot;#bibliography&quot;&gt;[3]&lt;/a&gt; when I felt uncomfortable. In what one might call “stepping out of one’s comfort zone”. And then, what used to scare me, started exciting me.&lt;/p&gt;

&lt;p&gt;To give a concrete example, chatting to complete strangers used to terrify me. Now I find it the most pleasant part of the day.&lt;/p&gt;

&lt;p&gt;The technique I used was a combination of breath control meditation I learned while studying Mindfulness &lt;a href=&quot;#bibliography&quot;&gt;[2]&lt;/a&gt;, followed by a simple visualization technique.&lt;/p&gt;

&lt;p&gt;Whenever an unpleasant event occured, I took a few deep breaths with my eyes closed, and I repeated a positive affirmation phrase (ie. “it’s ok, it’s not scary”). 
If the situation was really overwhelming, I seeked a safe place and launched into the meditation there. 
Then I actively seeked to repeat that experience and kept meditating through it, until I no longer responded emotionally.&lt;/p&gt;

&lt;p&gt;…I guess they are right when they say “get back on the horse that bucked you”.&lt;/p&gt;

&lt;h1 id=&quot;credits&quot;&gt;Credits&lt;/h1&gt;

&lt;p&gt;As I mentioned at the beginning, this wasn’t a lonely effort. 
I would like to thank each and every person I came across during this time. All of you had a huge impact on this investigation, and its ultimate success.&lt;/p&gt;

&lt;p&gt;My special thanks go to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Danusia, my wife - for all the love and support.&lt;/li&gt;
  &lt;li&gt;dr. Anna Mochnaczewska, my psychotherapist - for being a great and patient teacher.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you all from the bottom of my heart!&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;

&lt;p&gt;[1] “The role of the lateral amygdala in the retrieval and maintenance of fear-memories formed by repeated probabilistic reinforcement”, Jeffrey C. Erlich, David E. A. Bush, and Joseph E. LeDoux, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322351/&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322351/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Mindfulness, Wikipedia, &lt;a href=&quot;https://en.wikipedia.org/wiki/Mindfulness&quot;&gt;https://en.wikipedia.org/wiki/Mindfulness&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] “Rewire your Anxious Brain, how to use the neuroscience of fear to end anxiety, panic &amp;amp; worry”, Catherine M. Pittmann, Ph.D, Elizabeth M. Karle, MLIS; New Harbinger Publications Inc., 2015&lt;/p&gt;

&lt;p&gt;[4] “What Every Body is Saying”, Joe Navarro with Marcin Karlins, Ph.D., EPUB Edition, 2008&lt;/p&gt;</content><author><name>ptrochim</name></author><summary type="html">Emotions</summary></entry></feed>