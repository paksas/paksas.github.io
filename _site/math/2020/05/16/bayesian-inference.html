<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bayesian Inference | Piotr Trochim’s blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Bayesian Inference" />
<meta name="author" content="ptrochim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/math/2020/05/16/bayesian-inference.html" />
<meta property="og:url" content="http://localhost:4000/math/2020/05/16/bayesian-inference.html" />
<meta property="og:site_name" content="Piotr Trochim’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-16T00:00:00+01:00" />
<script type="application/ld+json">
{"headline":"Bayesian Inference","dateModified":"2020-05-16T00:00:00+01:00","datePublished":"2020-05-16T00:00:00+01:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/math/2020/05/16/bayesian-inference.html"},"url":"http://localhost:4000/math/2020/05/16/bayesian-inference.html","author":{"@type":"Person","name":"ptrochim"},"description":"Introduction","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Piotr Trochim's blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Piotr Trochim&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">Piotr</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Inference</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-05-16T00:00:00+01:00" itemprop="datePublished">May 16, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">ptrochim</span></span></p>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.</p>

<p><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood Estimate</a> method I covered in <a href="https://paksas.github.io/math/2020/05/15/mle.html">the previous post</a>, had its flaws, one of which being its ignorance to hypothesis that may shed light on the collected data. The method I will describe now addresses that very issue.</p>

<h1 id="back-alley-clown">Back alley clown</h1>

<p>Let me recall the example from the previous post.</p>

<blockquote>
  <p>Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.</p>

  <p>You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.</p>
</blockquote>

<p>Back then though, all we cared about was the result of those 10 throws. But that’s not how humans work. We take circumstance into account.
I would be far more likely to respond to the challenge if I was attending a fair than if I was making a shortcut through a dark, dirty alley.</p>

<p>I’m of course describing the process of <em>building confidence</em>.</p>

<p>A fair clown is something rather desired, whereas one encounteres in a back alley would have to first gain our trust. His coin tosses would have to be pretty <em>convincing</em>, showing no bias towards one side or another.</p>

<h1 id="two-distributions">Two distributions</h1>

<p>Mathematically speaking, we are still looking for the parameters of a distribution that models the Random Variable that is the clown’s coin.</p>

<p>However now in addition to solid evidence, we also have our <em>prior</em> beliefs. Those two combined allow us not only to make a guess about the parameter value, but also reflect our level of <em>confidence</em> in that guess.</p>

<p>If you recall, MLE direcly calculated the values of distribution parameters - it didn’t produce any value that would suggest a doubt about that value. It made sense, since the method relied exclusively on the data, which is a solid evidence. So unless there was other data available, the method was 100% confident about its estimate.</p>

<p>Right now we want to express our <em>doubt</em>, and <em>doubts</em> are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to <em>sample</em> the values of our <em>target distribution</em> parameters.</p>

<p>To reiterate:</p>

<ul>
  <li>MLE -&gt; <script type="math/tex">\theta</script></li>
  <li>Bayesian Inference -&gt; <script type="math/tex">\theta \sim P(\Theta)</script> (<script type="math/tex">\theta</script> sampled from a distribution <script type="math/tex">P</script> of the random variable <script type="math/tex">\Theta</script>)</li>
</ul>

<h1 id="bayesian-inference">Bayesian Inference</h1>

<p>As a reminder, the task of any parameter inference is to find parameters <script type="math/tex">\theta</script> based on some observed data <script type="math/tex">X</script>.
In the specific case of Bayesian Inference, <script type="math/tex">\theta</script> is estimated indirectly rather than being directly calculated.</p>

<p>This estimation is achieved by introducing a random variable <script type="math/tex">\Theta</script> that models the distribution of the parameter values.
Our task now is to calculate the values of <em>that</em> distribution, rather than to directly calculate the values of <script type="math/tex">\theta</script>, and use that parameter to parameterize the <em>target distributon</em> from which the data is sampled:</p>

<script type="math/tex; mode=display">\begin{cases} \theta \sim P(\Theta) \\ x \sim P(X | \Theta=\theta) \end{cases}</script>

<h1 id="bayes-theorem">Bayes theorem</h1>

<p>The method makes use of the famous <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.
It’s quite ubiquitous, and there is a ton of good material out there explaining it in great detail. Therefore I’m going to assume that you’re vaguely familiar with it as well as its main use cases.
If I may recommend a book that had the largest impact on my understanding, it would be: <a href=""></a></p>

<p>Let’s quickly recap the theorem and how can it lend itself ot our goal:</p>

<script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)*P(A)}{P(B)}</script>

<p>Let’s introduce its elements and the terminology:</p>
<ul>
  <li><script type="math/tex">P(A\|B)</script> is called <em>the posterior</em> and expresses our belief about <script type="math/tex">A</script> after observing <script type="math/tex">B</script>.</li>
  <li><script type="math/tex">P(A)</script> is called <em>the prior</em> and expresses our belief about <script type="math/tex">A</script> before observing <script type="math/tex">B</script>.</li>
  <li><script type="math/tex">P(B\|A)</script> is called <em>the likelihood</em> and expresses the way in which <script type="math/tex">B</script> affects our belief about <script type="math/tex">A</script>.</li>
  <li><script type="math/tex">P(B)</script> is sometimes called <em>the evidence</em> and functions as a normalizing factor.</li>
</ul>

<p>Reading this equation directly isn’t very informative. You immediately start asking the following questions:</p>
<ul>
  <li>if it’s <script type="math/tex">B</script> that we have observed (the data) and <script type="math/tex">A</script> is what we want to estimate, then how do we calculate <script type="math/tex">P(B\|A)</script>?</li>
  <li>..in fact, why don’t we calculate <script type="math/tex">P(A\|B)</script> directly using some distribution equation?</li>
  <li>what is this normalizing factor <script type="math/tex">P(B)</script> and how to calculate it?</li>
  <li>what distribution equations to use for all those terms?</li>
</ul>

<p>I want to focs on building thorough understanding of this before proceeding to the inference problem.</p>

<h2 id="notation">Notation</h2>

<h1 id="meaning-of-random-variables-a-and-b">Meaning of random variables A and B</h1>

<p>We want to estimate the values of parameters given some observed data, and the left hand side of Bayes theorem is:</p>

<script type="math/tex; mode=display">P(A|B)</script>

<p>That would make:</p>
<ul>
  <li><script type="math/tex">A</script> stand for the random variable that represents the parameters estimation <script type="math/tex">\Theta</script></li>
  <li><script type="math/tex">B</script> be the observed data <script type="math/tex">X</script></li>
</ul>

<p>Then we get:</p>

<script type="math/tex; mode=display">P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{P(X)}</script>

<p>Let’s recal our problem formulation. We are looking for a distribution parameterized by some value of <script type="math/tex">\theta</script>, from which the data samples (coin tosses) are drawn - <script type="math/tex">x \sim P(\theta)</script>.
But instead of calculating the value of <script type="math/tex">\theta</script> directly, we want to express our belief about its value, so we choose to model it with a random variable <script type="math/tex">\Theta</script>.
We are then going to use that random variable to parameterize the distribution we draw our data from:</p>

<script type="math/tex; mode=display">\begin{cases} \theta \sim P(\Theta) \\ x \sim P(X | \Theta=\theta) \end{cases}</script>

<p>Here, lowercase <script type="math/tex">x</script> and <script type="math/tex">\theta</script> denote the samples, and their uppercase equivalents <script type="math/tex">X</script> and <script type="math/tex">\Theta</script> - random variables.
Notice that <script type="math/tex">P(X | \Theta)</script> appears in the Bayesian equation, however not on its left hand side. It is in fact the likelihood.</p>

<p>This is very confusing. Are we calculating the wrong value? Should our equation look like this then:</p>

<script type="math/tex; mode=display">P(X | \Theta) = \frac{P(\Theta | X)*P(X)}{P(\Theta)}</script>

<p>The answer is NO. We are looking for P(\Theta), but the one that takes into account the observed data <script type="math/tex">X</script> - <script type="math/tex">P(\Theta | X)</script>.
It is that distribution that will express our belief about the parameterization of the data distribution.</p>

<p>But then how do we write down the <em>target data distribution</em>? We have several options, all of which definitely need to take <script type="math/tex">\Theta</script> into account as well as the random variable <script type="math/tex">X</script>, which models our data.
We know for sure that it’s NOT <script type="math/tex">P(\Theta)</script> - that’s the distribution of the parameters. So is it:</p>

<ul>
  <li><script type="math/tex">P(X, \Theta)</script> ?</li>
  <li><script type="math/tex">P(X \| \Theta)</script> ?</li>
  <li>or perhaps <script type="math/tex">P(X)</script> with an <em>invisible</em> <script type="math/tex">\Theta</script> parameterization everyone except for me sort of knows about?</li>
</ul>

<h1 id="distribution-function-notation">Distribution function notation</h1>

<p>Let’s see what different notations mean visually. To get nice visual results, I used a Normal distribution <script type="math/tex">N(\mu=0, \sigma=1)</script>:</p>

<table>
  <thead>
    <tr>
      <th>Notation</th>
      <th>Visual representation</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">P(X)</script></td>
      <td><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_1.png" alt="Fig 1" /></td>
      <td>1D probability mass/density function</td>
    </tr>
    <tr>
      <td><script type="math/tex">P(X=x)</script></td>
      <td><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_2.png" alt="Fig 2" /></td>
      <td>1D probability or density value</td>
    </tr>
    <tr>
      <td><script type="math/tex">P(X, Y)</script></td>
      <td><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_3.png" alt="Fig 3" /></td>
      <td>Joint (2D) probability mass/density function)</td>
    </tr>
    <tr>
      <td><script type="math/tex">P(X=x, Y=y)</script></td>
      <td><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_4.png" alt="Fig 4" /></td>
      <td>Probability or density value</td>
    </tr>
    <tr>
      <td><script type="math/tex">P(X, Y=y)</script> and <script type="math/tex">P(X | Y=y)</script></td>
      <td><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_5.png" alt="Fig 5" /></td>
      <td>A slice through a 2D function</td>
    </tr>
  </tbody>
</table>

<p>Notice how the conditional probability slices through the data. We could as well use the notation <script type="math/tex">P(X, Y=y)</script>, but the vertical bar notation was adopted for this purpose, and we call this slice “conditional” probability to emphasize that we are fixing the value of one of the variables.</p>

<p>Let’s see which type of notation best describe the <em>target distribution</em>:</p>

<ul>
  <li><script type="math/tex">P(X, \Theta)</script> - both the data <script type="math/tex">X</script> and the parameters <script type="math/tex">\Theta</script> haven’t been sampled, and we are talking about a joint function of both</li>
  <li><script type="math/tex">P(X \| \Theta)</script> - we have observed some value of <script type="math/tex">\Theta</script> and we want to use it to take a slice from <script type="math/tex">P(X, \Theta)</script>. We will consider this slice a 1D distribution of <script type="math/tex">X</script>.</li>
  <li><script type="math/tex">P(\Theta \| X)</script> - we have observed some value of <script type="math/tex">X</script> and we want to use it to take a slice from <script type="math/tex">P(X, \Theta)</script>. We will consider this slice a 1D distribution of <script type="math/tex">\Theta</script></li>
</ul>

<p>It’s clearly the 2nd one - <script type="math/tex">P(X \| \Theta)</script>. It feels like we’ve gone full circle - the likelihood term of the Bayes Theorem is denoted in the same way as the <em>target distribution</em> we will be sampling from.</p>

<p>This confused me for a very long time, and it stemmed from how nuanced the probability notation really is. Let’s looks at the equations side by side:</p>

<table>
  <thead>
    <tr>
      <th>Bayes Theorem</th>
      <th>Model of data generator</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{P(X)}</script></td>
      <td><script type="math/tex">x \sim P(X|\Theta)</script></td>
    </tr>
  </tbody>
</table>

<p>I searched long and hard for an explanation and, to my great disappointment, I didn’t find any. But I did find out a few things:</p>
<ul>
  <li><script type="math/tex">P(X\|\Theta)</script> in both cases can be read as <em>“conditional distribution of <script type="math/tex">X</script> given <script type="math/tex">\Theta</script>”</em></li>
  <li>In Bayes Theorem cast for finding the relationship between conditional distributions, that’s also what it is</li>
  <li>in Bayes Theorem cast for the inference problems, that conditional distribution isn’t a distribution at all - if you would integrate the area under its curve, that area wouldn’t equal 1.0.</li>
  <li>In out model of data generator, it should be read as the conditional distribution, but it’s a completely different instance of the distribution.</li>
</ul>

<p>It seems that the confusion stems from the fact that <script type="math/tex">P(X\|\Theta)</script> in the Bayesian Inference equation - the so-called <em>likelihood</em> term - isn’t a distribution at all. What is it then and why is it using the same notation as a distribution?</p>

<h1 id="likelihood-function">Likelihood function</h1>

<p>As we’ll see a bit later, the likelihood term is modeled using the equations of actual distribution functions, but it isn’t a distribution. The reason is that we are actually plugging in ALL of the values.
It should be written as <script type="math/tex">P(X=x|\Theta=\theta)</script> rather than <script type="math/tex">P(X|\Theta=\theta)</script>, which suggests <script type="math/tex">X</script> being a random variable and us modelling some sort of a distribution shape with it.</p>

<p>This function returns 2 different things, depending on the type of distribution function equation used:</p>

<table>
  <thead>
    <tr>
      <th>Distribution function type</th>
      <th>Likelihood return value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>discrete</td>
      <td>probability</td>
    </tr>
    <tr>
      <td>continuous</td>
      <td>density</td>
    </tr>
  </tbody>
</table>

<p>Probability density, because that’s the density I’m referring to, isn’t equivalent to the probability. But in case of a regular probability density function, should you integrate the area under the entire curve, it would equal to 1.0.</p>

<p>So why is it that in the case of likelihood it wouldn’t?</p>

<h1 id="the-parameters-of-the-likelihood-function">The parameters of the likelihood function</h1>

<p>The answer lies in the function support - another nuance of the notation we usually don’t bother to focus on at all.</p>

<p>As I mentioned before, the likelihood term is expressed using a regular distribution function equation. I want to use a concrete example to drive the point here, and since we’re talking about coin tosses, I’m going to use the Binomial distribution equation. But please keep in mind that this applies to ANY distribution just the same.</p>

<p>The equation of Binomial distribution probability <em>mass</em> function: <script type="math/tex">f(k, n, p) = \binom{n}{k} p^{k}(1-p)^{n-k}</script>. It’s a discrete distribution.
It’s discreteness stems from the fact that its support is a subset of integers defined as <script type="math/tex">k \in {0..n}</script>.</p>

<p>One would use this equation to express a distribution of probability by choosing some value of <script type="math/tex">n</script> and <script type="math/tex">p</script>, and then iterating over all possible values of <script type="math/tex">k</script> and running the three through this equation.
Let’s say that we want to model the distribution of getting k Heads in 10 tosses, and the tosses are performed with a fair coin (p=0.5).
The equation would then take the form:</p>

<script type="math/tex; mode=display">f(10, k, 0.5) = \binom{10}{k} 0.5^{k}(1-0.5)^{10-k}</script>

<p><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_6.png" alt="Fig 6" /></p>

<p>If you would integrate (sum up in this case) the values, the result would equal 1.0.</p>

<p>But <script type="math/tex">k</script> isn’t the only parameter we can iterate over. What would happen if we fixed it, and iterated over all possible values of <script type="math/tex">p</script> instead?
To be more specific - we have performed 10 tosses, 3 of them were Heads. How does the probability change for different values of <script type="math/tex">p</script> ?</p>

<script type="math/tex; mode=display">f(10, 3, p) = \binom{10}{3} p^{3}(1-p)^{10-3} = 120 p^{3}(1-p)^{7}</script>

<p><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_7.png" alt="Fig 7" /></p>

<p>Notice a few things off the bat:</p>
<ul>
  <li>the plot is no longer choppy.</li>
  <li>it has a different range - (0, 1) rather than {0..10}</li>
  <li>the integral of this function equals 0.09, instead of 1, which should be the case if this was a proper distribution function.</li>
</ul>

<p>Bayesian Inference writes the likelihood as if it was something that can be modelled as a joint distribution of two variables - the data and the parameterization. Specifically:</p>

<p><script type="math/tex">f(n, k, p) = P(X \| \Theta)</script> where <script type="math/tex">X: k</script> and <script type="math/tex">\Theta: (n, p)</script>.</p>

<p>The only justification I can think of at the time of writing this article is that this is a notation abuse.
As we’ve seen in the case of Binomial distribution function (and other functions follow suite), there is a very specific support defined and one can’t just turn other parameters into random variables and expect to obtain a joint distributon.</p>

<p>In addition to causing confusion, it also introduces mathematical discrepancy - the nominator can no longer be treated as a distribution. So in order to bring the posterior back on track to being a regular distribution, another term needs to step in and act as a normalizer.</p>

<h1 id="the-normalizing-role-of-the-evidence-term">The normalizing role of the evidence term.</h1>

<p>Going back to the Bayes Theorem, we find the evidence defined as <script type="math/tex">P(X)</script>.
The extended version of the Theorem rewrites this term as <script type="math/tex">P(X) = \int P(X | \Theta=\theta) d\theta</script>:</p>

<script type="math/tex; mode=display">P(\Theta|X) = \frac{P(X|\Theta)*P(\Theta)}{\int P(X | \Theta=\theta) d\theta}</script>

<p>It means that we will marginalize random variable <script type="math/tex">X</script>, which represents the model of our data, for all possible values of <script type="math/tex">\Theta</script>. That marginalization would guarantee a distribution, if <script type="math/tex">P(X\|\Theta)</script> was in fact a distribution, which it isn’t.</p>

<p>However - it will normalize the nominator such that the calculated posterior P(\Theta | X) will be a distribution! This does not depend on the type of distribution equations used.
I would however appreciate a proof, so if anyone knows it, please share.</p>

<h1 id="can-maximum-likelihood-estimate-be-used-to-calculate-the-likelihood-term">Can Maximum Likelihood Estimate be used to calculate the Likelihood term?</h1>

<p>To close the topic of confusing notation, I wanted to bring up a strange similarity of names between the Maximum Likelihood Estimate and the Likelihood term in the Bayesian Inference.
What confused me in particular was whether MLE <em>should</em> be used to calculate the value returned by the likelihood term.</p>

<p>There is no relation between the two:</p>
<ul>
  <li>As we’ve learned, the likelihood term returns some value or function, which shouldn’t be treated as a probability distribution.</li>
  <li>The Maximum Likelihood Estimate is a method to assess the value of distribution parameter <script type="math/tex">\theta</script>.</li>
</ul>

<p>The two return completely different things. In fact, on the conceptual level, MLE produces the same values that sampling from the posterior returned by Bayesian Inference does:</p>

<script type="math/tex; mode=display">\begin{cases} \theta \sim MLE(X) \\ \theta \sim P(\Theta | X) \end{cases}</script>

<h2 id="how-does-it-work">How does it work</h2>

<p>Now that we know how each part of the equation works, I wanted to show how it can be used.</p>

<p>Allow me to pull some fake data out of the hat and use it to solve our coin tossing problem.
For example - let’s say that I’ve observed the clown tossing 7 Heads in 10 coin tosses.
As a reminder, I now need to call the result of the next toss. In order to do that, I need to estimate the “fairness” of the coin, or our parameter <script type="math/tex">p</script> that we have seen used in the Binomial distribution:</p>

<script type="math/tex; mode=display">f(n, k, p) = \binom{n}{k}p^k(1-p)^{n-k}</script>

<p>Once I know the value of that parameter, I will be able to determine which side is more likely to show up on the next toss. If the value is closer to 1 - it will be the Heads, if it’s closer to 0 - Tails.
The value of 0.5 will put me in a pickle, because it would mean the coin is fair and I would have to guess.</p>

<p>Maximum Likelihood Estimate would suggest the coin is biased towards Heads. However, I have some other information I can incorporate in my assessment.</p>

<h1 id="choosing-a-prior">Choosing a prior</h1>

<p>My first clue is that I encountered the clown while taking a shortcut through a dark back alley. Yes, this isn’t a fair, or any other place where you would expect to meet a coin tossing clown.
I suspect that he will try to trick me and he’s somehow controlling the coin.</p>

<p>My suspicion is that it’s biased towards Tails.</p>

<p>I will use a Beta distribution:</p>

<script type="math/tex; mode=display">P(\Theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha - 1}(1-\theta)^{\beta - 1}</script>

<p>with parameters <script type="math/tex">alpha=1</script> and <script type="math/tex">beta=3</script> as a prior, which I think best reflects that belief:</p>

<script type="math/tex; mode=display">P(\Theta) = \frac{\Gamma(4)}{\Gamma(1)\Gamma(3)}\theta^{0}(1-\theta)^{2} = 3 (1-\theta)^{2}</script>

<p><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_8.png" alt="Fig 8" /></p>

<h1 id="choosing-the-likelihood-function">Choosing the likelihood function</h1>

<p>The likelihood function needs to appraise the collected data and return a <em>metric</em> (not a probability, but a metric), how likely that data was given the hypothesized parameter values.
The choice of the function is arbitrary, and often dictated by a similarity to a particular distribution model.</p>

<p>That is also why I chose to use the Binomial Distribution function with fixed parameters <script type="math/tex">(n, k)</script> as the likelihood function:</p>

<script type="math/tex; mode=display">P(X\|\Theta) = \binom{10}{7}\theta^{7}(1-\theta)^3=120[\theta^{7}(1-\theta)^3]</script>

<p><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_9.png" alt="Fig 9" /></p>

<h1 id="calculating-the-evidence-denominator">Calculating the evidence denominator</h1>

<p>Let’s start with the integral version of the Bayes Theorem denominator. <script type="math/tex">\theta</script> here has support in range (0, 1), which we will use as the integration limits.</p>

<script type="math/tex; mode=display">P(X) = \int_{0}^{1} P(X\|\Theta) * P(\Theta) d\theta</script>

<p>Substitute the functions:</p>

<script type="math/tex; mode=display">P(X) = \int_{0}^{1} [3(1-\theta)^{2}] [120[\theta^{7}(1-\theta)^3]] d\theta</script>

<p>Extract the constants and combine the variables, and then use Wolfram Alpha to calculate the final result:</p>

<script type="math/tex; mode=display">P(X) = 360 \int_{0}^{1} \theta^{7}(1-\theta)^{5} d\theta = \frac{5}{143}</script>

<p>Notice that this result is a constant. That’s the normalizing constant I was referring to.</p>

<h1 id="put-it-all-together">Put it all together</h1>

<p>We can now substitute those equations into the Bayes Theorem equation:</p>

<script type="math/tex; mode=display">P(\Theta \| X) = \frac{P(X|\Theta)P(\Theta)}{P(X)} = \frac{[120[\theta^{7}(1-\theta)^3]][3 (1-\theta)^{2}]}{\frac{5}{143}}</script>

<p>After combining the terms, we get:</p>

<script type="math/tex; mode=display">P(\Theta \| X) = 10296 \theta^{7}(1-\theta)^5</script>

<p><img src="https://github.com/paksas/paksas.github.io/raw/master/_images/bayes_infr_fig_10.png" alt="Fig 10" /></p>

<p>And just to verify that it actually represents a probability distribution, its integral in <script type="math/tex">\theta</script> support equals 1:</p>

<script type="math/tex; mode=display">\int_{0}^{1} 10296 \theta^{7}(1-\theta)^5 = 1</script>

<h1 id="conjugate-priors">Conjugate priors</h1>

<p>Explain that in order to be able to use posterior as the next iteration prior, MLE and prior need to be conjugate distributions
Quote Pearse stats book on the conjugate prior
Beta is a conjugate of the binomial distr.</p>


  </div><a class="u-url" href="/math/2020/05/16/bayesian-inference.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Piotr Trochim&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Piotr Trochim&#39;s blog</li><li><a class="u-email" href="mailto:ptrochim@gmail.com">ptrochim@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/paksas"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">paksas</span></a></li><li><a href="https://www.twitter.com/piotrtrochim"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">piotrtrochim</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Thoughts and results I wish to persist.
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
