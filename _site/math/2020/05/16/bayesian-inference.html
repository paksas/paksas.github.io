<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bayesian Inference | Piotr Trochim’s blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Bayesian Inference" />
<meta name="author" content="ptrochim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/math/2020/05/16/bayesian-inference.html" />
<meta property="og:url" content="http://localhost:4000/math/2020/05/16/bayesian-inference.html" />
<meta property="og:site_name" content="Piotr Trochim’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-16T00:00:00+01:00" />
<script type="application/ld+json">
{"headline":"Bayesian Inference","dateModified":"2020-05-16T00:00:00+01:00","datePublished":"2020-05-16T00:00:00+01:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/math/2020/05/16/bayesian-inference.html"},"url":"http://localhost:4000/math/2020/05/16/bayesian-inference.html","author":{"@type":"Person","name":"ptrochim"},"description":"Introduction","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Piotr Trochim's blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Piotr Trochim&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">Piotr</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Inference</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-05-16T00:00:00+01:00" itemprop="datePublished">May 16, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">ptrochim</span></span></p>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>This is a post in the series about statistical model fitting.
It’s my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.</p>

<p>In this post, I will be covering a method that builds on top of <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood Estimate</a> method I covered in <a href="https://paksas.github.io/math/2020/05/15/mle.html">the previous post</a>, enhancing it.</p>

<h1 id="back-alley-clown">Back alley clown</h1>

<p>Let me recall the example from the previous post.</p>

<blockquote>
  <p>Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge.</p>

  <p>You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.</p>
</blockquote>

<p>Back then though, all we cared about was the result of those 10 throws. But that’s not how humans work. We take circumstance into account.
I would be far more likely to respond to the challenge if I was attending a fair than if I was making a shortcut through a dark, dirty alley.</p>

<p>I’m of course describing the process of <em>building confidence</em>.</p>

<p>A fair clown is something rather desired, whereas one encounteres in a back alley would have to first gain our trust. His coin tosses would have to be pretty <em>convincing</em>, showing no bias towards one side or another.</p>

<h1 id="two-distributions">Two distributions</h1>

<p>Mathematically speaking, we are still looking for the parameters of a distribution that models the Random Variable that is the clown’s coin.</p>

<p>However now in addition to solid evidence, we also have our <em>prior</em> beliefs. Those two combined allow us not only to make a guess about the parameter value, but also reflect our level of <em>confidence</em> in that guess.</p>

<p>If you recall, MLE allowed to direcly calculate the values of distribution parameters. It meant that it was caclulating a distribution with <em>complete confidence</em> in its result. It makes sense, since the method relied exclusively on the data, which is a solid evidence. So unless there was other data available, the method was 100% confident about its estimate.</p>

<p>Right now we want to express our <em>doubt</em>, and <em>doubts</em> are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to <em>sample</em> the values of our <em>target distribution</em> parameters.</p>

<p>It took me a very long time to come to that realization during my studies, so I want to repeat that <em>important</em> paragraph again:</p>

<blockquote>
  <p>Right now we want to express our <em>doubt</em>, and <em>doubts</em> are expressed using… distributions.
That’s right - we are going to find another distribution, which we will use to <em>sample</em> the values of our <em>target distribution</em> parameters.</p>
</blockquote>

<h1 id="building-a-model-of-uncertainty">Building a model of uncertainty</h1>

<p>We have two quantities now:</p>
<ul>
  <li>the observed coin toss results</li>
  <li>the level of confidence about the parameters</li>
</ul>

<p>We can express confidence about something with a distribution. Let’s denote that distribution <script type="math/tex">\xi</script>, without defining it really.
Ditributions are properties of Random Variables - and this one is no different. Its Random Variable samples from a set of all possible distribution parameters <script type="math/tex">\theta \in \omega</script>.</p>

<p>Think about it - we now have a random variable that has some expected value and variance (uncertainty about that value) that represents the parameters <script type="math/tex">\theta</script> of our <em>target distribution</em>. This is exactly what we wanted.</p>

<p>But what about the data - the toss results?
Well, since we have access to <script type="math/tex">\theta</script> values, we could check which ones can even generate the tosses we observed, and how likely would those be.</p>

<p>Then, we could weight those predictions by the probability of actually drawing the specific parameter values <script type="math/tex">\theta</script>.</p>

<p>Allow me to be loose with the notation and just start putting things together:</p>

<script type="math/tex; mode=display">p(\theta, data) = f(data, \theta) * \xi(\theta)</script>

<p>If <script type="math/tex">\theta</script> is a specific sample, and so is the <script type="math/tex">data</script>:</p>
<ul>
  <li><script type="math/tex">\xi</script> would return the probability of drawing <script type="math/tex">\theta</script> from the <em>parameter distribution</em>.</li>
  <li><script type="math/tex">f(data, \theta)</script> would return the probability of drawing <script type="math/tex">data</script> for the given value of <script type="math/tex">\theta</script></li>
  <li>and by that property, <script type="math/tex">p(\theta, data)</script> would be the revised <em>“probability”</em> of drawing parameter value <script type="math/tex">\theta</script> after having observed <script type="math/tex">data</script>.</li>
</ul>

<p>There are 3 problems with the equation above:</p>
<ul>
  <li>it operates on specific values, so it could be good maybe for discrete distributions, but would break as soon as we wanted to use a continuous one.</li>
  <li>it operates on specific values (deuce), so it doesn’t really give us the distribution we are looking for</li>
  <li>if <script type="math/tex">\xi</script> was a discrete distribution, and we would actually iterate all values of <script type="math/tex">\xi</script> and summed the returned probabilites, they wouldn’t add up to 1 - so it doesn’t actually return a probability distribution.</li>
</ul>

<h1 id="bayes-theorem">Bayes Theorem</h1>

<p>At this point I would like to introduce the Bayes Theorem. Actually all of you probably know it, and those who don’t can have a quick look at one of the numerous sources that describe it in detail, such as <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">this Wikipedia page</a>.</p>

<script type="math/tex; mode=display">p(A|B)=\frac{p(B|A) * p(A)}{p(B)}</script>

<p>or, more precisely:</p>

<script type="math/tex; mode=display">p(A_j|B)=\frac{p(B|A_j) * p(A_j)}{\sum_{i} p(B|A_i) * p(A_i)}</script>

<p>In plain English, our belief about particular parameter values A_j, after we have observed coin toss results B, is the same as the probability of seeing those results being drawn from a distribution parameterized using A_j, and then scaling that value by the likelihood of actually coming across the parameters A_j, as well as that of running into thoe toss results - no matter what parameterization is used.</p>

<h1 id="bayes-theorem-to-fix-our-model">Bayes Theorem to fix our model</h1>

<p>Our model doesn’t look much like the Bayes Theorem:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Bayes Theorem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">p(\theta, data) = f(data, \theta) * \xi(\theta)</script></td>
      <td><script type="math/tex">p(A|B)=\frac{p(B|A) * p(A)}{p(B)}</script></td>
    </tr>
  </tbody>
</table>

<p>Even if we replace A and B with the same letters, they are still different:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Bayes Theorem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">p(\theta, data) = f(data, \theta) * \xi(\theta)</script></td>
      <td><script type="math/tex">p(\Theta|Data)=\frac{p(Data|\Theta) * p(\Theta)}{p(Data)}</script></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>we don’t have the denominator <script type="math/tex">p(Data)</script>.</li>
  <li>Bayes Theorem uses Random Variables (upper case labels), where as our model operates on samples (lower case labels)</li>
  <li>the functions look kind of the same, but the ones in our model are missing the conditional vertical bars</li>
</ul>

<h1 id="conjugate-distribution">Conjugate distribution</h1>

<p>Explain that in order to be able to use posterior as the next iteration prior, MLE and prior need to be conjugate distributions
Quote Pearse stats book on the conjugate prior
Beta is a conjugate of the binomial distr.</p>

<h1 id="posterior">Posterior</h1>

<h1 id="algebra-to-derive-the-posterior-update-rule">Algebra to derive the posterior update rule</h1>

<h1 id="example">Example</h1>

<p>Interactive plot showing a few iterations in which we update the initial belief</p>

<p>here’s my plot</p>

<html lang="en">
  <head>
      <meta charset="utf-8" />
      <title>my plot</title>  
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="ca2a4bcc-13e4-4a2e-b5de-4cfc587902a5" data-root-id="1001"></div>
            
          
        
      
      
        <script type="application/json" id="1146">
          {"85c49e6c-531d-4827-98bd-4723a9986045":{"roots":{"references":[{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"1044","type":"BoxAnnotation"},{"attributes":{},"id":"1025","type":"HelpTool"},{"attributes":{},"id":"1045","type":"Selection"},{"attributes":{},"id":"1024","type":"ResetTool"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"value":"#1f77b4"},"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"x":{"field":"x"},"y":{"field":"y"}},"id":"1035","type":"Circle"},{"attributes":{},"id":"1016","type":"BasicTicker"},{"attributes":{},"id":"1008","type":"LinearScale"},{"attributes":{},"id":"1046","type":"UnionRenderers"},{"attributes":{"source":{"id":"1033","type":"ColumnDataSource"}},"id":"1037","type":"CDSView"},{"attributes":{},"id":"1023","type":"SaveTool"},{"attributes":{"callback":null,"data":{"x":[1,2],"y":[3,4]},"selected":{"id":"1045","type":"Selection"},"selection_policy":{"id":"1046","type":"UnionRenderers"}},"id":"1033","type":"ColumnDataSource"},{"attributes":{"dimension":1,"ticker":{"id":"1016","type":"BasicTicker"}},"id":"1019","type":"Grid"},{"attributes":{"fill_color":{"value":"#1f77b4"},"line_color":{"value":"#1f77b4"},"x":{"field":"x"},"y":{"field":"y"}},"id":"1034","type":"Circle"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"1020","type":"PanTool"},{"id":"1021","type":"WheelZoomTool"},{"id":"1022","type":"BoxZoomTool"},{"id":"1023","type":"SaveTool"},{"id":"1024","type":"ResetTool"},{"id":"1025","type":"HelpTool"}]},"id":"1026","type":"Toolbar"},{"attributes":{"formatter":{"id":"1040","type":"BasicTickFormatter"},"ticker":{"id":"1011","type":"BasicTicker"}},"id":"1010","type":"LinearAxis"},{"attributes":{"data_source":{"id":"1033","type":"ColumnDataSource"},"glyph":{"id":"1034","type":"Circle"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"1035","type":"Circle"},"selection_glyph":null,"view":{"id":"1037","type":"CDSView"}},"id":"1036","type":"GlyphRenderer"},{"attributes":{},"id":"1011","type":"BasicTicker"},{"attributes":{"callback":null},"id":"1004","type":"DataRange1d"},{"attributes":{},"id":"1020","type":"PanTool"},{"attributes":{"text":""},"id":"1039","type":"Title"},{"attributes":{},"id":"1006","type":"LinearScale"},{"attributes":{"ticker":{"id":"1011","type":"BasicTicker"}},"id":"1014","type":"Grid"},{"attributes":{"callback":null},"id":"1002","type":"DataRange1d"},{"attributes":{},"id":"1021","type":"WheelZoomTool"},{"attributes":{},"id":"1040","type":"BasicTickFormatter"},{"attributes":{"formatter":{"id":"1042","type":"BasicTickFormatter"},"ticker":{"id":"1016","type":"BasicTicker"}},"id":"1015","type":"LinearAxis"},{"attributes":{},"id":"1042","type":"BasicTickFormatter"},{"attributes":{"overlay":{"id":"1044","type":"BoxAnnotation"}},"id":"1022","type":"BoxZoomTool"},{"attributes":{"below":[{"id":"1010","type":"LinearAxis"}],"center":[{"id":"1014","type":"Grid"},{"id":"1019","type":"Grid"}],"left":[{"id":"1015","type":"LinearAxis"}],"renderers":[{"id":"1036","type":"GlyphRenderer"}],"title":{"id":"1039","type":"Title"},"toolbar":{"id":"1026","type":"Toolbar"},"x_range":{"id":"1002","type":"DataRange1d"},"x_scale":{"id":"1006","type":"LinearScale"},"y_range":{"id":"1004","type":"DataRange1d"},"y_scale":{"id":"1008","type":"LinearScale"}},"id":"1001","subtype":"Figure","type":"Plot"}],"root_ids":["1001"]},"title":"Bokeh Application","version":"1.4.0"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('1146').textContent;
                  var render_items = [{"docid":"85c49e6c-531d-4827-98bd-4723a9986045","roots":{"1001":"ca2a4bcc-13e4-4a2e-b5de-4cfc587902a5"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>

<p>after the plot</p>

  </div><a class="u-url" href="/math/2020/05/16/bayesian-inference.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Piotr Trochim&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Piotr Trochim&#39;s blog</li><li><a class="u-email" href="mailto:ptrochim@gmail.com">ptrochim@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/paksas"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">paksas</span></a></li><li><a href="https://www.twitter.com/piotrtrochim"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">piotrtrochim</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Thoughts and results I wish to persist.
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
