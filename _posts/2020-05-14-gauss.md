 ---
title: "Inference from data"
author: ptrochim
categories: math
---

# Introduction

I was trying to understand how to find distributions that correspond to observed data.
As I started reading literature, I soon got lost between references to Maximum Likelihood Estimates, Bayesian estimators, Gaussian processes, regressions, distributions, density function and a ton of other terms all fed at once, often without any context or relationship between one another whatsoever.

TL;DR: they are related.
They are subsequent steps in a progression to build a mechanism that finds a function responsible for the observed sample points.

So let me try to explain it the easy way. 

# Is this a fair coin?

Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a guy flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge. 

You need to guess if the coin is fair or not. He will throw and catch the coin 10 times, telling you the result of the throw every single time.
The sooner you make the guess, the more money you collect. And if you linger for too long, and the 10th throw is complete, you don't get a dime.

And as he starts throwing the coin, you start observing the first results...

# What You See Is All There Is (WYSIATI)

This famous misnomer coined by (...) in their book "Thinking Fast and Slow" does give a hint how to proceed.

If you had observed 4 heads, it would mean you're dealing with a coin biased towards the heads.
If you had observed 2 heads and 2 tails, you would conclude that it's a fair coin.

In order to quantify this knowledge, we can say that are loking at both:
* the number of observations $$n$$
* the quality of the observations $${x_0, x_1, ..., x_n}$$

And it's only when *on average* we get an equal chance of throwing Heads or Tails, do we conclude that the coin is fair.

$$average = \frac{1, n} \sum {i=0, n} x_i$$ [eq 1]
 

# Maximum Likelihood Estimate

We would also arrive at [eq 1] if we applied a techinque called Maximum Likelihood Estimate (or MLE for short).


# Uncertainty

But what happens at odd number of readings. You are clearly going to be dealing with some bias.
Let's say that you've observed 3 heads and 2 tails.


