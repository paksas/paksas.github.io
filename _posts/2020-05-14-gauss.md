---
title: "Maximum Likelihood Estimate"
mathjax: true
author: ptrochim
categories: math
---

# Introduction

This is a post in the series about statistical model fitting.
It's my attempt to explain various approaches and algorithms in a way I would have liked to learn them when I had first started studying this fascinating topic.

In this post, I will be covering an algorithm that relies exclusively on the data and doesn't require the user to use any auxiliary distributions - Maximum Likelihood Estimate.

I will assume that you are familiar with:
* The concept of a random variable
* Binomial distribution
* Expected value of a random variable

# Is this a fair coin?

Imagine walking down the street one day, and seeing a small crowd of people surrounding what appears to be a clown flipping a coin.
He notices you as you approach and waves at you with his other hand. Then, he presents you with a challenge. 

You need to guess if toss coin is fair or not. He will toss the coin 10 times, telling you the result of each toss.
He will then toss it one more time, and you need to guess the result.

# What You See Is All There Is (WYSIATI)

This famous mnemonic coined by Daniel Kahneman in his book "Thinking Fast and Slow" gives a hint how to proceed.

What would be your guess if you have observed 10 Heads, and you had no other information to go on?

If you did't see the greedy smirk on the clown's face, nor the long face of the person walking away from the table. You only saw the clown toss 10 Heads in a row.
What would be the result of the next one?

Clearly, it'd be Heads. And if he threw 10 Tails, you would be stupid not to have bet on Tails... would you?

As these thoughts run through your head, you are trying to guess a certain property - what to *expect* from the coin as you keep tossing it:
* 10 Heads - $$E[X] = 1$$, so bet Heads
* 10 Tails - $$E[X] = 0$$, so bet Tails
* 5 Heads and 5 Tails - $$E[X] = 0.5$$, so... make a guess :/

We need to guess the *expected value* of the Random Variable, otherwise known as the mean of its distribution.

# Relating data to a Random Variable

Our coin tossing experiment is best represented by a Random Variable with a Binomial Distribution.

A discrete Random Variable is a sampler that pulls items out of a set. How frequently will certain values show up is dictated by its probability (mass) function. And it's expected value (or mean) indicates which particular item we can expect to see pulled out the most.

But how do we reverse it? How do we guess the mean and parameterize the probability (mass) function given the data that has been pulled out?

# Discrete or continuous is a matter of perspective

The expected value of R.V. (a.k.a the mean of its distribution) is a maxima of that R.V's probability (mass) function.
The best way to find the maxima of a function is to solve its derivative for its roots !

So let's remind ourselves of Binomial distribution p.m.f:

$$f(\theta, n, k) = \binom{n}{k} \theta^{n} (1 - \theta)^{n-k}$$

Where:
* $$n$$ - number of *positive* samples (in our case let's assume those are represented by Heads)
* $$k$$ - total number of samples
* $$\theta$$ - a latent parameter that represents the fairness of our coin (in this particular case).

Binomial distribution is a discrete distribution. The word discrete suggests something that's not continuous, something we wouldn't be able to differentiate.
But whether it's discrete or continuous depends on what we consider its *variable*.

While calculating probabilities of events, we usually are given some fixed $$\theta$$ (we are throwing a specific coin), and we operate in terms of $$n$$ and $$k$$ (or sometimes only the latter).
These two are discrete positive values, and that's what the distribution derives its name from.

In our case however, these two parameters are fixed - we are given 10 observations ($$n$$), and $$k$$ of them are Heads. What we need to find is such value of $$\theta$$ that will maximize our observations:

$$ \frac{df(\theta, n, k)}{d\theta} = 0 $$

# TODO: graphical representation

# Solving the derivative

Since $$n$$ and $$k$$ are constants, we might get rid of the ominous binomial $$\binom{n}{k}$$, by the property of dividing both sides of equation by it (zero divided by whatever remains a zero).

$$ \frac{df(\theta, n, k)}{d\theta} = n\theta^{n-1}(1-\theta)^{n-k} + \theta^{n}(n-k)(1-\theta)^{n-k-1} = 0$$

and after [a little bit of algebra](#algebraic-appendix), we get $$ \theta = \frac{n}{k}$$

The result suggests that the parameter of Binomial Distribution's probability mass function that maximizes the likelihood of observing clown's coin toss results is... the ratio of obtained Heads $$n$$ to the number of performed throws $$n$$. 

# Deeper insight - the average

If we assumed that Tails are represented by 0, and Heads by a 1, and an $$x_i$$ would represent $$i^{th}$$ coin toss result, then $$n = \sum_{i=0}^{n} x_i$$.

Our result then becomes the *average* value of the observed data:

$$ \theta = \frac{n}{k} = \frac{1}{k} \sum_{i=0}^{n} x_i $$

# Maximum Likelihood Estimate

The method the relies on finding the maxima of a probability mass/density function is called Maximum Likelihood Estimation.
It's major feature is its independence. The only thing we care about when using it is the data.

That unfortunately is also one of its major weaknesses, as we'll see in the subsequent posts about the Bayesian methods.

# Algebraic appendix

I'll flex my algebra muscle, but I'll try explaining what inspired specific transformations.
The equation contains a plus sign, which won't allow us to use division as a way of getting rid of unwanted stuff that easily. So my first hunch is to rearrange the terms.
That way I can use the equals sign as a mirror and check if there are any symmetrical terms - terms that occur on both sides that I can *divide away*.

$$ n\theta^{n-1}(1-\theta)^{n-k} = -\theta^{n}(n-k)(1-\theta)^{n-k-1}$$

The next transformation becomes immediately obvious.

$$\theta^{n}$$ can be broken down into $$\theta^{n-1} * \theta$$ which gives us $$\theta^{n-1}$$ on both sides of the $$=$$ sign. The same transformation can be applied to (1-\theta)^{n-k}

$$ n\theta^{n-1}(1-\theta)^{n-k-1}(1-\theta) = -\theta^{n-1}\theta(n-k)(1-\theta)^{n-k-1}$$

Now we can divide them away, and we get:

$$ n(1-\theta) = -\theta(n-k)$$

If we unroll the terms in the parenthesis, we get the term $$-n\theta$$ on boths sides of the $$=$$ sign.

$$ n - n\theta = -n\theta + k\theta$$

After removing it, we get:

$$ n = k\theta$$

And that leads straight to:

$$ \theta = \frac{n}{k}$$

